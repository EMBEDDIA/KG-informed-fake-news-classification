{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "golden-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import san\n",
    "from src_end2end import statistical_features\n",
    "import lsa_features\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from sklearn import preprocessing\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from sklearn.metrics import f1_score\n",
    "st_models = [\"roberta-large-nli-stsb-mean-tokens\", \"xlm-r-large-en-ko-nli-ststb\", \"distilbert-base-nli-mean-tokens\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fantastic-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st_models = [\"roberta-large-nli-stsb-mean-tokens\", \"xlm-r-large-en-ko-nli-ststb\", \"distilbert-base-nli-mean-tokens\"]\n",
    "def embedd_bert(text, st_model = 'paraphrase-distilroberta-base-v1', split = 'train'):    \n",
    "    paths = \"temp_berts/\"+st_model+\"_\"+split+'.pkl'\n",
    "    if os.path.isfile(paths):\n",
    "        sentence_embeddings = pickle.load(open(paths,'rb')) \n",
    "        return sentence_embeddings\n",
    "    model = SentenceTransformer(st_model)\n",
    "    sentence_embeddings = model.encode(text)\n",
    "    with open(paths, 'wb') as f:\n",
    "        pickle.dump(sentence_embeddings, f)\n",
    "    return sentence_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "human-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st_models = [\"roberta-large-nli-stsb-mean-tokens\", \"xlm-r-large-en-ko-nli-ststb\", \"distilbert-base-nli-mean-tokens\"]\n",
    "def embedd_bert2(text, st_model = 'paraphrase-distilroberta-base-v1'):    \n",
    "    text = [t[:512] for t in text]\n",
    "    model = SentenceTransformer(st_model)\n",
    "    sentence_embeddings = model.encode(text)\n",
    "    return sentence_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nearby-testament",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "for dataset in tqdm([\"pan2020\", \"AAAI2021_COVID19_fake_news\", \"LIAR_PANTS\", \"ISOT\", \"FakeNewsNet\"]):\n",
    "    path = \"representations/\"+dataset+\"/\"\n",
    "    for thing in [\"train\", \"dev\", \"test\"]:\n",
    "        path_in = \"data/final/\"+dataset+\"/\"+thing+'.csv'\n",
    "        df = pd.read_csv(path_in, encoding='utf-8')\n",
    "       # texts[thing] = df.text_a.to_list()\n",
    "        ys = df.label.to_list()\n",
    "      \n",
    "        path_tmp = path + thing + \"/\" + \"_ys.csv\"\n",
    "        np.savetxt(path_tmp, ys, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "proper-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"pan2020\"\n",
    "texts = {}\n",
    "ys = {}\n",
    "for thing in [\"train\", \"dev\", \"test\"]:\n",
    "    path_in = \"data/final/\"+dataset+\"/\"+thing+'.csv'\n",
    "    df = pd.read_csv(path_in, encoding='utf-8')\n",
    "    texts[thing] = df.text_a.to_list()\n",
    "    ys[thing] = df.label.to_list()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "lesser-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:20<00:00, 70.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for dataset in tqdm([\"LIAR\", \"FakeNewsNet\"]):#\"pan2020\", \"ISOT\", \"AAAI2021_COVID19_fake_news\", \"LIAR\", \"FakeNewsNet\"]):\n",
    "    export_kgs_spec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "contained-federation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'export_kgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9dd789646128>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pan2020\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AAAI2021_COVID19_fake_news\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LIAR_PANTS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ISOT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FakeNewsNet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mexport_kgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'export_kgs' is not defined"
     ]
    }
   ],
   "source": [
    "for dataset in tqdm([\"pan2020\", \"AAAI2021_COVID19_fake_news\", \"LIAR_PANTS\", \"ISOT\", \"FakeNewsNet\"]):\n",
    "    export_kgs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "based-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_kgs(dataset):\n",
    "    path = \"representations/\"+dataset+\"/\"\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        for kg in [\"complex\", \"transe\", \"quate\", \"simple\", \"rotate\", \"distmult\"]:\n",
    "            path_tmp = path + split + \"/\" + kg + \".csv\"\n",
    "            tmp_kg = prep_kgs(kg, split)\n",
    "            tmp_kg = np.array((tmp_kg))\n",
    "            np.savetxt(path_tmp, tmp_kg, delimiter=\",\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "lyric-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_kgs_spec(dataset):\n",
    "    path = \"representations/\"+dataset+\"/\"\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        for kg in [\"complex\", \"transe\", \"quate\", \"simple\", \"rotate\", \"distmult\"]:\n",
    "            path_tmp = path + split + \"/\" + kg + \"_entity.csv\"\n",
    "            tmp_kg = prep_kgs2(kg, split)\n",
    "            tmp_kg = np.array((tmp_kg))\n",
    "            np.savetxt(path_tmp, tmp_kg, delimiter=\",\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "patient-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_LM(dataset):\n",
    "    global texts, ys\n",
    "    path = \"representations/\"+dataset+\"/\"\n",
    "    for thing in [\"train\", \"dev\", \"test\"]:\n",
    "        path_in = \"data/final/\"+dataset+\"/\"+thing+'.csv'\n",
    "        df = pd.read_csv(path_in, encoding='utf-8')\n",
    "        texts[thing] = df.text_a.to_list()\n",
    "        ys[thing] = df.label.to_list()\n",
    "        staticstical = statistical_features.fit_space(texts[thing])\n",
    "        kg = 'stat'\n",
    "        path_tmp = path + thing + \"/\" + kg + \".csv\"\n",
    "        np.savetxt(path_tmp, staticstical, delimiter=\",\")\n",
    "        \n",
    "        bertz = embedd_bert2(texts[thing], st_models[0])\n",
    "        kg = st_models[0]\n",
    "        path_tmp = path + thing + \"/\" + kg + \".csv\"\n",
    "        np.savetxt(path_tmp, bertz, delimiter=\",\")\n",
    "        \n",
    "        bertz2 = embedd_bert2(texts[thing], st_models[1]) \n",
    "        kg = st_models[1]\n",
    "        path_tmp = path + thing + \"/\" + kg + \".csv\"\n",
    "        np.savetxt(path_tmp, bertz2, delimiter=\",\")\n",
    "         \n",
    "        bertz3 = embedd_bert2(texts[thing], st_models[2]) \n",
    "        kg = st_models[2]\n",
    "        path_tmp = path + thing + \"/\" + kg + \".csv\"\n",
    "        np.savetxt(path_tmp, bertz3, delimiter=\",\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "fleet-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_kgs(kg_emb, split='train'):\n",
    "    embs = []\n",
    "    global dataset\n",
    "    path_in = \"kg_emb_dump/\"+dataset+\"/\"+split+\"_\"+kg_emb+'.pkl'\n",
    "    with open(path_in, \"rb\") as f:\n",
    "        kgs_p = pickle.load(f)\n",
    "    for x,y in kgs_p:\n",
    "        embs.append(y)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for dataset in tqdm([\"LIAR_PANTS\"]):#\"pan2020\", \"ISOT\", \"AAAI2021_COVID19_fake_news\", \"LIAR\", \"FakeNewsNet\"]):\n",
    "    export_LM(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "located-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_kgs2(kg_emb, split='train'):\n",
    "    embs = []\n",
    "    global dataset\n",
    "    path_in = \"kg_emb_dump/\"+dataset+\"/\"+split+\"_\"+kg_emb+'_speakers.pkl'\n",
    "    with open(path_in, \"rb\") as f:\n",
    "        kgs_p = pickle.load(f)\n",
    "    for x,y in kgs_p:\n",
    "        embs.append(y)\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "tender-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_all_feats(kg_emb, lsa_feats, lsa_dims, split = \"train\"):\n",
    "    global texts\n",
    "    LM_feats_train = prep_features_textual(texts[split], lsa_feats, lsa_dims, split)\n",
    "    kg_feats_train = prep_kgs(kg_emb, split)\n",
    "    final_feats = np.hstack((LM_feats_train, kg_feats_train))\n",
    "    return preprocessing.scale(final_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "modular-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_features_textual(texts, lsa_feats, lsa_dims, split):   \n",
    "    staticstical = statistical_features.fit_space(texts)\n",
    "    bertz = embedd_bert(texts, st_models[0], split) \n",
    "    bertz2 = embedd_bert(texts, st_models[1], split) \n",
    "    bertz3 = embedd_bert(texts, st_models[2], split) \n",
    "    lsa = lsa_features.fit(texts)\n",
    "    s_b = np.hstack((staticstical, bertz))\n",
    "    s_b = np.hstack((s_b, bertz2))\n",
    "    s_b = np.hstack((s_b, bertz3))\n",
    "    s_b = np.hstack((s_b, lsa))\n",
    "    return s_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "optimum-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"AAAI2021_COVID19_fake_news\"\n",
    "kgs_p = {}\n",
    "for thing in [\"train\", \"dev\", \"test\"]:\n",
    "    path_in = \"kg_emb_dump/\"+dataset+\"/\"+thing+'.pkl'\n",
    "    with open(path_in, \"rb\") as f:\n",
    "        kgs_p[thing] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = {}\n",
    "for thing in kgs_p:\n",
    "    embs[thing] = []\n",
    "    for x,y in kgs_p[thing]:\n",
    "        embs[thing].append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "Xs = {}\n",
    "ys = {}\n",
    "for thing in [\"train\", \"dev\", \"test\"]:\n",
    "    ys[thing] = []\n",
    "    for c in dfs[thing][\"label\"].to_list():\n",
    "        if c == 'real':\n",
    "            ys[thing].append(0)\n",
    "        else:\n",
    "            ys[thing].append(1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "front-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dfs[\"train\"].text_a.to_list()\n",
    "x_dev = dfs[\"dev\"].text_a.to_list()\n",
    "x_test = dfs[\"test\"].text_a.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "overhead-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dev =  preprocessing.scale(np.hstack((feats_dev, embs[\"dev\"])))\n",
    "final_test =  preprocessing.scale(np.hstack((feats_test, embs[\"test\"])))\n",
    "final_train =  preprocessing.scale(np.hstack((feats_train, embs[\"train\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dev = prep_features_textual(x_dev, 1500, 512)\n",
    "feats_test = prep_features_textual(x_test, 1500, 512)\n",
    "feats_train = prep_features_textual(x_train, 1500, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "alpha-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392592592592593"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(max_iter = 10000).fit(final_train, ys[\"train\"])\n",
    "preds = clf.predict(final_dev)\n",
    "f1_score(ys[\"dev\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "grand-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAN\n",
    "num_epochs = None\n",
    "num_heads = None\n",
    "batch_size = None\n",
    "dropout = None\n",
    "hidden_lyr_size = None\n",
    "learning_rate = None    \n",
    "#LSA\n",
    "lsa_n_features = None\n",
    "lsa_n_dimensions = None\n",
    "#KG\n",
    "kg_id_mask = None\n",
    "\n",
    "\n",
    "bo_num_epochs = Integer(low=1, high=5, name='bo_num_epochs')\n",
    "bo_num_heads = Integer(low=1, high=4, name='bo_num_heads')\n",
    "bo_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform', name='bo_learning_rate')\n",
    "bo_hidden_lyr_size = Categorical([512,1024,2048,4096], name = 'bo_hidden_lyr_size')\n",
    "bo_batch_size = Categorical([32, 64, 128, 256, 512], name='bo_batch_size')\n",
    "bo_dropout = Real(low=0.1, high=0.9, prior='uniform', name='bo_dropout')\n",
    "#[1250, 2500, 5000, 10000, 15000, 20000]\n",
    "bo_lsa_n_features = Categorical([2500], name='bo_lsa_n_features')\n",
    "#[64,128, 256, 512]\n",
    "bo_lsa_n_dimensions = Categorical([512], name='bo_lsa_n_dimensions')\n",
    "\n",
    "bo_kg_type = Categorical([\"transe\", \"quate\", \"simple\", \"rotate\", \"distmult\", \"complex\"], name = 'bo_kg_type')# \"distmult\", \"complex\", \"simple\", \"rotate\", \"quate\"], name = 'bo_kg_type')\n",
    "\n",
    "dimensions = [bo_lsa_n_features, bo_lsa_n_dimensions, \n",
    "              bo_num_epochs, bo_num_heads, bo_hidden_lyr_size, bo_batch_size, bo_learning_rate, bo_dropout,\n",
    "              bo_kg_type]\n",
    "\n",
    "\n",
    "default_parameters = [2500, 512,\n",
    "                      3, 2, 2048, 256, 0.0001, 0.3948527313133352, \"transe\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "polyphonic-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logs_file = \"logs/outs_\"+dataset+\".log\"\n",
    "logs_file_ptr = open(logs_file, 'w')\n",
    "#logging.basicConfig(filename=logs_file, encoding='utf-8', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "brutal-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(bo_lsa_n_features, bo_lsa_n_dimensions, \n",
    "              bo_num_epochs, bo_num_heads, bo_hidden_lyr_size, bo_batch_size, bo_learning_rate, bo_dropout, bo_kg_type):\n",
    "\n",
    "    global iteration, num_steps, lstm_size, init_epoch, max_epoch, learning_rate_decay, dropout_rate, init_learning_rate, batch_size, kd_id_mask\n",
    "\n",
    "    #LSA\n",
    "    lsa_n_features = int(bo_lsa_n_features)\n",
    "    lsa_n_dimensions = int(bo_lsa_n_dimensions)\n",
    "    #SAN\n",
    "    num_epochs = int(bo_num_epochs)\n",
    "    num_heads = int(bo_num_heads)\n",
    "    batch_size = int(bo_batch_size)\n",
    "    dropout = float(bo_dropout)\n",
    "    hidden_lyr_size = int(bo_num_epochs)\n",
    "    learning_rate = float(bo_learning_rate)    \n",
    "    #KG\n",
    "    kg_id_mask = bo_kg_type        \n",
    "    clf = san.SAN(num_epochs = num_epochs, num_heads = num_heads, batch_size = batch_size, dropout = dropout, learning_rate = learning_rate, hidden_layer_size = hidden_lyr_size)\n",
    "    #PREP FEATS\n",
    "    logging.info('Preprearing train features')\n",
    "    x_train = prep_all_feats(kg_id_mask, lsa_n_features, lsa_n_dimensions, 'train') \n",
    "    logging.info('Finished train features')\n",
    "\n",
    "\n",
    "    clf.fit(x_train, ys[\"train\"])\n",
    "    del x_train\n",
    "    \n",
    "    logging.info('Preprearing dev features')\n",
    "    x_dev = prep_all_feats(kg_id_mask, lsa_n_features, lsa_n_dimensions, 'dev') \n",
    "    predict_dev = clf.predict(x_dev)\n",
    "    del x_dev\n",
    "   \n",
    "    logging.info('Finished dev features')\n",
    "\n",
    "    dev_ys = ys[\"dev\"]    \n",
    "    dev_error = 1 - f1_score(dev_ys, predict_dev, average='weighted')\n",
    "    print(\"DEV F1-score\", 1 - dev_error)\n",
    "    logging.info('DEV F1-score: ' + str(1-dev_error))\n",
    "    \n",
    "    log_line = [dev_error, bo_lsa_n_features, bo_lsa_n_dimensions, bo_num_epochs, bo_num_heads, bo_hidden_lyr_size, bo_batch_size, bo_learning_rate, bo_dropout, bo_kg_type]\n",
    "    log_line_s = '\\t'.join([str(s) for s in log_line])\n",
    "    logs_file_ptr.write(log_line_s+\"\\n\")\n",
    "    do_test = False\n",
    "    if do_test:\n",
    "        logging.info('Preprearing test features')\n",
    "        x_test = prep_all_feats(kg_id_mask, lsa_n_features, lsa_n_dimensions, 'test') \n",
    "        logging.info('Finished test features')\n",
    "\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_err = 1 - f1_score(ys[\"test\"], predict_test, average='weighted')\n",
    "        logging.info('TEST F1-score: ' + str(1-test_err))\n",
    "\n",
    "        print(\"TEST  F1-score\", 1 - test_err)\n",
    "    \n",
    "    return dev_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "elementary-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:07:00 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:07:36 - Finished train features\n",
      "12-Mar-21 11:07:36 - Found 2 unique labels.\n",
      "12-Mar-21 11:07:37 - Number of parameters 50750315\n",
      "12-Mar-21 11:07:37 - Starting training for 3 epochs\n",
      "12-Mar-21 11:07:37 - epoch 0, mean loss per batch 0.704490453004837\n",
      "12-Mar-21 11:07:38 - epoch 1, mean loss per batch 0.5989087820053101\n",
      "12-Mar-21 11:07:39 - epoch 2, mean loss per batch 0.5340623259544373\n",
      "12-Mar-21 11:07:39 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:07:51 - Finished dev features\n",
      "12-Mar-21 11:07:51 - DEV F1-score: 0.5895355001835162\n",
      "12-Mar-21 11:07:51 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.5895355001835162\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:08:25 - Finished train features\n",
      "12-Mar-21 11:08:25 - Found 2 unique labels.\n",
      "12-Mar-21 11:08:25 - Number of parameters 67667086\n",
      "12-Mar-21 11:08:25 - Starting training for 4 epochs\n",
      "12-Mar-21 11:08:26 - epoch 0, mean loss per batch 0.7910850860855796\n",
      "12-Mar-21 11:08:27 - epoch 1, mean loss per batch 1.2226759412071921\n",
      "12-Mar-21 11:08:28 - epoch 2, mean loss per batch 1.263328189199621\n",
      "12-Mar-21 11:08:28 - epoch 3, mean loss per batch 1.0509942065585742\n",
      "12-Mar-21 11:08:28 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:08:41 - Finished dev features\n",
      "12-Mar-21 11:08:41 - DEV F1-score: 0.39748508247286307\n",
      "12-Mar-21 11:08:41 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.39748508247286307\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:09:14 - Finished train features\n",
      "12-Mar-21 11:09:14 - Found 2 unique labels.\n",
      "12-Mar-21 11:09:14 - Number of parameters 84579742\n",
      "12-Mar-21 11:09:14 - Starting training for 4 epochs\n",
      "12-Mar-21 11:09:16 - epoch 0, mean loss per batch 0.7010332345962524\n",
      "12-Mar-21 11:09:17 - epoch 1, mean loss per batch 9.490750312805176\n",
      "12-Mar-21 11:09:18 - epoch 2, mean loss per batch 2.2053017616271973\n",
      "12-Mar-21 11:09:19 - epoch 3, mean loss per batch 0.8750849366188049\n",
      "12-Mar-21 11:09:19 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:09:31 - Finished dev features\n",
      "12-Mar-21 11:09:31 - DEV F1-score: 0.45137732602142\n",
      "12-Mar-21 11:09:31 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.45137732602142\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:10:04 - Finished train features\n",
      "12-Mar-21 11:10:04 - Found 2 unique labels.\n",
      "12-Mar-21 11:10:04 - Number of parameters 33837659\n",
      "12-Mar-21 11:10:04 - Starting training for 3 epochs\n",
      "12-Mar-21 11:10:05 - epoch 0, mean loss per batch 0.6929201781749725\n",
      "12-Mar-21 11:10:06 - epoch 1, mean loss per batch 0.6099933485190073\n",
      "12-Mar-21 11:10:06 - epoch 2, mean loss per batch 0.5947411259015402\n",
      "12-Mar-21 11:10:06 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:10:18 - Finished dev features\n",
      "12-Mar-21 11:10:18 - DEV F1-score: 0.6524154493198854\n",
      "12-Mar-21 11:10:18 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.6524154493198854\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:10:51 - Finished train features\n",
      "12-Mar-21 11:10:51 - Found 2 unique labels.\n",
      "12-Mar-21 11:10:51 - Number of parameters 67667086\n",
      "12-Mar-21 11:10:51 - Starting training for 4 epochs\n",
      "12-Mar-21 11:10:52 - epoch 0, mean loss per batch 18.27263880859722\n",
      "12-Mar-21 11:10:53 - epoch 1, mean loss per batch 24.830454046075996\n",
      "12-Mar-21 11:10:53 - epoch 2, mean loss per batch 26.163263841108844\n",
      "12-Mar-21 11:10:54 - epoch 3, mean loss per batch 23.953123352744363\n",
      "12-Mar-21 11:10:54 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:11:06 - Finished dev features\n",
      "12-Mar-21 11:11:06 - DEV F1-score: 0.3382613222280062\n",
      "12-Mar-21 11:11:06 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.3382613222280062\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:11:39 - Finished train features\n",
      "12-Mar-21 11:11:39 - Found 2 unique labels.\n",
      "12-Mar-21 11:11:39 - Number of parameters 67658856\n",
      "12-Mar-21 11:11:39 - Starting training for 2 epochs\n",
      "12-Mar-21 11:11:40 - epoch 0, mean loss per batch 0.751345306634903\n",
      "12-Mar-21 11:11:41 - epoch 1, mean loss per batch 0.6454722583293915\n",
      "12-Mar-21 11:11:41 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:11:53 - Finished dev features\n",
      "12-Mar-21 11:11:53 - DEV F1-score: 0.4629927594529364\n",
      "12-Mar-21 11:11:53 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.4629927594529364\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:12:25 - Finished train features\n",
      "12-Mar-21 11:12:25 - Found 2 unique labels.\n",
      "12-Mar-21 11:12:26 - Number of parameters 67658856\n",
      "12-Mar-21 11:12:26 - Starting training for 2 epochs\n",
      "12-Mar-21 11:12:27 - epoch 0, mean loss per batch 2.1959924697875977\n",
      "12-Mar-21 11:12:27 - epoch 1, mean loss per batch 0.6175179481506348\n",
      "12-Mar-21 11:12:27 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:12:39 - Finished dev features\n",
      "12-Mar-21 11:12:39 - DEV F1-score: 0.43108133945154603\n",
      "12-Mar-21 11:12:39 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.43108133945154603\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:13:12 - Finished train features\n",
      "12-Mar-21 11:13:12 - Found 2 unique labels.\n",
      "12-Mar-21 11:13:13 - Number of parameters 67662971\n",
      "12-Mar-21 11:13:13 - Starting training for 3 epochs\n",
      "12-Mar-21 11:13:13 - epoch 0, mean loss per batch 12.473230570554733\n",
      "12-Mar-21 11:13:14 - epoch 1, mean loss per batch 6.9583441615104675\n",
      "12-Mar-21 11:13:15 - epoch 2, mean loss per batch 1.38889479637146\n",
      "12-Mar-21 11:13:15 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:13:26 - Finished dev features\n",
      "12-Mar-21 11:13:26 - DEV F1-score: 0.3284285489867519\n",
      "12-Mar-21 11:13:26 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.3284285489867519\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:13:59 - Finished train features\n",
      "12-Mar-21 11:13:59 - Found 2 unique labels.\n",
      "12-Mar-21 11:14:00 - Number of parameters 50746200\n",
      "12-Mar-21 11:14:00 - Starting training for 2 epochs\n",
      "12-Mar-21 11:14:00 - epoch 0, mean loss per batch 0.8740585988218134\n",
      "12-Mar-21 11:14:01 - epoch 1, mean loss per batch 0.7313899018547751\n",
      "12-Mar-21 11:14:01 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:14:13 - Finished dev features\n",
      "12-Mar-21 11:14:13 - DEV F1-score: 0.34777743871427114\n",
      "12-Mar-21 11:14:13 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.34777743871427114\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:14:46 - Finished train features\n",
      "12-Mar-21 11:14:46 - Found 2 unique labels.\n",
      "12-Mar-21 11:14:46 - Number of parameters 67667086\n",
      "12-Mar-21 11:14:46 - Starting training for 4 epochs\n",
      "12-Mar-21 11:14:47 - epoch 0, mean loss per batch 0.6992225646972656\n",
      "12-Mar-21 11:14:48 - epoch 1, mean loss per batch 2.6648693084716797\n",
      "12-Mar-21 11:14:48 - epoch 2, mean loss per batch 0.8764280080795288\n",
      "12-Mar-21 11:14:49 - epoch 3, mean loss per batch 0.71159827709198\n",
      "12-Mar-21 11:14:49 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:15:01 - Finished dev features\n",
      "12-Mar-21 11:15:01 - DEV F1-score: 0.3284285489867519\n",
      "12-Mar-21 11:15:01 - Preprearing train features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.3284285489867519\n",
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:15:34 - Finished train features\n",
      "12-Mar-21 11:15:34 - Found 2 unique labels.\n",
      "12-Mar-21 11:15:35 - Number of parameters 67658856\n",
      "12-Mar-21 11:15:35 - Starting training for 2 epochs\n",
      "12-Mar-21 11:15:35 - epoch 0, mean loss per batch 0.8642154534657797\n",
      "12-Mar-21 11:15:36 - epoch 1, mean loss per batch 0.6656755606333414\n",
      "12-Mar-21 11:15:36 - Preprearing dev features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "url\n",
      "hash\n",
      "sw\n",
      "pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 11:15:48 - Finished dev features\n",
      "12-Mar-21 11:15:48 - DEV F1-score: 0.4540028064475923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.4540028064475923\n"
     ]
    }
   ],
   "source": [
    "search_result = gp_minimize(func=fitness,\n",
    "                                dimensions=dimensions,\n",
    "                                acq_func='EI', # Expected Improvement.\n",
    "                                n_calls=11,\n",
    "                                n_jobs = -1,\n",
    "                                x0=default_parameters,\n",
    "                                random_state=42)\n",
    "import pickle\n",
    "with open(\"bayes_\"+dataset+\".opt\", \"wb\") as f:\n",
    "    pickle.dump(search_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "studied-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "import pickle\n",
    "with open(\"bayes_\"+dataset+\".opt\", \"rb\") as f:\n",
    "    search_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "useful-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "registered-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500, 512, 3, 1, 4096, 64, 0.00018699039697141505, 0.5947088074664699, 'quate']\n",
      "0.3475845506801146\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(search_result.x)\n",
    "print(search_result.fun)\n",
    "plot = plot_convergence(search_result,yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "interracial-integrity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12-Mar-21 08:43:17 - Found 2 unique labels.\n",
      "12-Mar-21 08:43:18 - Number of parameters 60407240\n",
      "12-Mar-21 08:43:18 - Starting training for 722 epochs\n",
      "12-Mar-21 08:43:19 - epoch 0, mean loss per batch 0.34796875657943577\n",
      "12-Mar-21 08:43:21 - epoch 1, mean loss per batch 0.06861647848899548\n",
      "12-Mar-21 08:43:22 - epoch 2, mean loss per batch 0.013529577937263709\n",
      "12-Mar-21 08:43:24 - epoch 3, mean loss per batch 0.004278961434745445\n",
      "12-Mar-21 08:43:25 - epoch 4, mean loss per batch 0.0020166456412810544\n",
      "12-Mar-21 08:43:27 - epoch 5, mean loss per batch 0.00129849163036292\n",
      "12-Mar-21 08:43:28 - epoch 6, mean loss per batch 0.000963668804615736\n",
      "12-Mar-21 08:43:29 - epoch 7, mean loss per batch 0.0007562956956322663\n",
      "12-Mar-21 08:43:31 - epoch 8, mean loss per batch 0.0005833762339674509\n",
      "12-Mar-21 08:43:32 - epoch 9, mean loss per batch 0.0004797438752855389\n",
      "12-Mar-21 08:43:34 - epoch 10, mean loss per batch 0.00041483253545056167\n",
      "12-Mar-21 08:43:35 - epoch 11, mean loss per batch 0.0003437941217830834\n",
      "12-Mar-21 08:43:36 - epoch 12, mean loss per batch 0.0002944947224862587\n",
      "12-Mar-21 08:43:38 - epoch 13, mean loss per batch 0.00025735532052259747\n",
      "12-Mar-21 08:43:39 - epoch 14, mean loss per batch 0.00022977075814896336\n",
      "12-Mar-21 08:43:41 - epoch 15, mean loss per batch 0.00020022570242872462\n",
      "12-Mar-21 08:43:42 - epoch 16, mean loss per batch 0.00017903201408514107\n",
      "12-Mar-21 08:43:43 - epoch 17, mean loss per batch 0.00016524935423289068\n",
      "12-Mar-21 08:43:45 - epoch 18, mean loss per batch 0.00014692503184330865\n",
      "12-Mar-21 08:43:46 - epoch 19, mean loss per batch 0.0001382620911709427\n",
      "12-Mar-21 08:43:48 - epoch 20, mean loss per batch 0.00012450045995557538\n",
      "12-Mar-21 08:43:49 - epoch 21, mean loss per batch 0.00011432662134981141\n",
      "12-Mar-21 08:43:51 - epoch 22, mean loss per batch 0.00010272931215765241\n",
      "12-Mar-21 08:43:52 - epoch 23, mean loss per batch 9.494331327284902e-05\n",
      "12-Mar-21 08:43:53 - epoch 24, mean loss per batch 8.946261676520898e-05\n",
      "12-Mar-21 08:43:55 - epoch 25, mean loss per batch 8.239473222951907e-05\n",
      "12-Mar-21 08:43:56 - epoch 26, mean loss per batch 7.975472628738946e-05\n",
      "12-Mar-21 08:43:58 - epoch 27, mean loss per batch 7.217815408904034e-05\n",
      "12-Mar-21 08:43:59 - epoch 28, mean loss per batch 6.852861378529969e-05\n",
      "12-Mar-21 08:44:00 - epoch 29, mean loss per batch 6.450394133795411e-05\n",
      "12-Mar-21 08:44:02 - epoch 30, mean loss per batch 6.082575656854225e-05\n",
      "12-Mar-21 08:44:03 - epoch 31, mean loss per batch 5.558655832898624e-05\n",
      "12-Mar-21 08:44:04 - epoch 32, mean loss per batch 5.253789011196484e-05\n",
      "12-Mar-21 08:44:06 - epoch 33, mean loss per batch 4.998672319253763e-05\n",
      "12-Mar-21 08:44:07 - epoch 34, mean loss per batch 4.6364026834803204e-05\n",
      "12-Mar-21 08:44:09 - epoch 35, mean loss per batch 4.419617233417319e-05\n",
      "12-Mar-21 08:44:10 - epoch 36, mean loss per batch 4.2959830282667936e-05\n",
      "12-Mar-21 08:44:11 - epoch 37, mean loss per batch 3.975843292909513e-05\n",
      "12-Mar-21 08:44:13 - epoch 38, mean loss per batch 3.842047702234525e-05\n",
      "12-Mar-21 08:44:14 - epoch 39, mean loss per batch 3.702687032413311e-05\n",
      "12-Mar-21 08:44:16 - epoch 40, mean loss per batch 3.428996548721513e-05\n",
      "12-Mar-21 08:44:17 - epoch 41, mean loss per batch 3.263310705173913e-05\n",
      "12-Mar-21 08:44:19 - epoch 42, mean loss per batch 3.1577962545270566e-05\n",
      "12-Mar-21 08:44:20 - epoch 43, mean loss per batch 3.070872270864829e-05\n",
      "12-Mar-21 08:44:22 - epoch 44, mean loss per batch 2.933820199429577e-05\n",
      "12-Mar-21 08:44:23 - epoch 45, mean loss per batch 2.809154249497134e-05\n",
      "12-Mar-21 08:44:24 - epoch 46, mean loss per batch 2.6128202674814158e-05\n",
      "12-Mar-21 08:44:26 - epoch 47, mean loss per batch 2.5385607631715873e-05\n",
      "12-Mar-21 08:44:27 - epoch 48, mean loss per batch 2.4388075242033945e-05\n",
      "12-Mar-21 08:44:29 - epoch 49, mean loss per batch 2.4383243166644555e-05\n",
      "12-Mar-21 08:44:30 - epoch 50, mean loss per batch 2.208783143942128e-05\n",
      "12-Mar-21 08:44:32 - epoch 51, mean loss per batch 2.2265270042855544e-05\n",
      "12-Mar-21 08:44:33 - epoch 52, mean loss per batch 2.12679859452272e-05\n",
      "12-Mar-21 08:44:35 - epoch 53, mean loss per batch 1.9993988364428962e-05\n",
      "12-Mar-21 08:44:36 - epoch 54, mean loss per batch 1.9325577026253224e-05\n",
      "12-Mar-21 08:44:38 - epoch 55, mean loss per batch 1.8676466984638515e-05\n",
      "12-Mar-21 08:44:39 - epoch 56, mean loss per batch 1.7755739218745686e-05\n",
      "12-Mar-21 08:44:41 - epoch 57, mean loss per batch 1.7737954504599867e-05\n",
      "12-Mar-21 08:44:42 - epoch 58, mean loss per batch 1.6643553898085465e-05\n",
      "12-Mar-21 08:44:43 - epoch 59, mean loss per batch 1.6313618988743445e-05\n",
      "12-Mar-21 08:44:45 - epoch 60, mean loss per batch 1.551687705614313e-05\n",
      "12-Mar-21 08:44:46 - epoch 61, mean loss per batch 1.5181319842947862e-05\n",
      "12-Mar-21 08:44:48 - epoch 62, mean loss per batch 1.4976632498403957e-05\n",
      "12-Mar-21 08:44:49 - epoch 63, mean loss per batch 1.473652547736912e-05\n",
      "12-Mar-21 08:44:51 - epoch 64, mean loss per batch 1.3458728517434793e-05\n",
      "12-Mar-21 08:44:52 - epoch 65, mean loss per batch 1.3507082223021336e-05\n",
      "12-Mar-21 08:44:53 - epoch 66, mean loss per batch 1.3154811558006959e-05\n",
      "12-Mar-21 08:44:55 - epoch 67, mean loss per batch 1.268230177559487e-05\n",
      "12-Mar-21 08:44:56 - epoch 68, mean loss per batch 1.2461191965065575e-05\n",
      "12-Mar-21 08:44:58 - epoch 69, mean loss per batch 1.164535842690384e-05\n",
      "12-Mar-21 08:44:59 - epoch 70, mean loss per batch 1.1236599483219414e-05\n",
      "12-Mar-21 08:45:01 - epoch 71, mean loss per batch 1.119822281207934e-05\n",
      "12-Mar-21 08:45:02 - epoch 72, mean loss per batch 1.071749681194398e-05\n",
      "12-Mar-21 08:45:04 - epoch 73, mean loss per batch 1.0706619902451235e-05\n",
      "12-Mar-21 08:45:05 - epoch 74, mean loss per batch 1.0247973897583926e-05\n",
      "12-Mar-21 08:45:06 - epoch 75, mean loss per batch 9.7373591730642e-06\n",
      "12-Mar-21 08:45:08 - epoch 76, mean loss per batch 9.635330081017366e-06\n",
      "12-Mar-21 08:45:09 - epoch 77, mean loss per batch 9.229552495945916e-06\n",
      "12-Mar-21 08:45:11 - epoch 78, mean loss per batch 9.143684547570256e-06\n",
      "12-Mar-21 08:45:12 - epoch 79, mean loss per batch 8.789698872883142e-06\n",
      "12-Mar-21 08:45:14 - epoch 80, mean loss per batch 8.657717811589604e-06\n",
      "12-Mar-21 08:45:15 - epoch 81, mean loss per batch 8.719531828878993e-06\n",
      "12-Mar-21 08:45:16 - epoch 82, mean loss per batch 8.168198326949585e-06\n",
      "12-Mar-21 08:45:18 - epoch 83, mean loss per batch 8.084230697792149e-06\n",
      "12-Mar-21 08:45:19 - epoch 84, mean loss per batch 7.895797107966455e-06\n",
      "12-Mar-21 08:45:21 - epoch 85, mean loss per batch 7.5595998378748145e-06\n",
      "12-Mar-21 08:45:22 - epoch 86, mean loss per batch 7.3250276255538875e-06\n",
      "12-Mar-21 08:45:24 - epoch 87, mean loss per batch 7.188432671402832e-06\n",
      "12-Mar-21 08:45:25 - epoch 88, mean loss per batch 6.89956984318157e-06\n",
      "12-Mar-21 08:45:27 - epoch 89, mean loss per batch 7.003962187249831e-06\n",
      "12-Mar-21 08:45:28 - epoch 90, mean loss per batch 6.598754846611253e-06\n",
      "12-Mar-21 08:45:29 - epoch 91, mean loss per batch 6.623906266703181e-06\n",
      "12-Mar-21 08:45:31 - epoch 92, mean loss per batch 6.435228285156952e-06\n",
      "12-Mar-21 08:45:32 - epoch 93, mean loss per batch 6.21593468307625e-06\n",
      "12-Mar-21 08:45:34 - epoch 94, mean loss per batch 5.958097115301196e-06\n",
      "12-Mar-21 08:45:35 - epoch 95, mean loss per batch 5.917273990864767e-06\n",
      "12-Mar-21 08:45:36 - epoch 96, mean loss per batch 5.85061546259497e-06\n",
      "12-Mar-21 08:45:38 - epoch 97, mean loss per batch 5.578520690855489e-06\n",
      "12-Mar-21 08:45:39 - epoch 98, mean loss per batch 5.686852038986846e-06\n",
      "12-Mar-21 08:45:41 - epoch 99, mean loss per batch 5.473714754071597e-06\n",
      "12-Mar-21 08:45:42 - epoch 100, mean loss per batch 5.2597617048797056e-06\n",
      "12-Mar-21 08:45:44 - epoch 101, mean loss per batch 5.133475444661319e-06\n",
      "12-Mar-21 08:45:45 - epoch 102, mean loss per batch 5.07016740742144e-06\n",
      "12-Mar-21 08:45:46 - epoch 103, mean loss per batch 5.067028592538219e-06\n",
      "12-Mar-21 08:45:48 - epoch 104, mean loss per batch 4.93419104084471e-06\n",
      "12-Mar-21 08:45:49 - epoch 105, mean loss per batch 4.880178319301474e-06\n",
      "12-Mar-21 08:45:51 - epoch 106, mean loss per batch 4.614516840844702e-06\n",
      "12-Mar-21 08:45:52 - epoch 107, mean loss per batch 4.555483245055187e-06\n",
      "12-Mar-21 08:45:53 - epoch 108, mean loss per batch 4.344211325028132e-06\n",
      "12-Mar-21 08:45:55 - epoch 109, mean loss per batch 4.420939586242849e-06\n",
      "12-Mar-21 08:45:56 - epoch 110, mean loss per batch 4.2392665066542504e-06\n",
      "12-Mar-21 08:45:58 - epoch 111, mean loss per batch 4.130946570932372e-06\n",
      "12-Mar-21 08:45:59 - epoch 112, mean loss per batch 3.996935377542216e-06\n",
      "12-Mar-21 08:46:01 - epoch 113, mean loss per batch 4.017801590914762e-06\n",
      "12-Mar-21 08:46:02 - epoch 114, mean loss per batch 3.86403484334061e-06\n",
      "12-Mar-21 08:46:03 - epoch 115, mean loss per batch 3.7452589455055063e-06\n",
      "12-Mar-21 08:46:05 - epoch 116, mean loss per batch 3.764120378946245e-06\n",
      "12-Mar-21 08:46:06 - epoch 117, mean loss per batch 3.6050027154907675e-06\n",
      "12-Mar-21 08:46:08 - epoch 118, mean loss per batch 3.6709020168228353e-06\n",
      "12-Mar-21 08:46:09 - epoch 119, mean loss per batch 3.519213036130308e-06\n",
      "12-Mar-21 08:46:10 - epoch 120, mean loss per batch 3.3887775605133074e-06\n",
      "12-Mar-21 08:46:12 - epoch 121, mean loss per batch 3.4094665317332747e-06\n",
      "12-Mar-21 08:46:13 - epoch 122, mean loss per batch 3.2833515800890523e-06\n",
      "12-Mar-21 08:46:15 - epoch 123, mean loss per batch 3.292197885247543e-06\n",
      "12-Mar-21 08:46:16 - epoch 124, mean loss per batch 3.1047226404832773e-06\n",
      "12-Mar-21 08:46:17 - epoch 125, mean loss per batch 3.2256980900386293e-06\n",
      "12-Mar-21 08:46:19 - epoch 126, mean loss per batch 2.983377804901885e-06\n",
      "12-Mar-21 08:46:20 - epoch 127, mean loss per batch 2.929665345601069e-06\n",
      "12-Mar-21 08:46:22 - epoch 128, mean loss per batch 2.913180441492631e-06\n",
      "12-Mar-21 08:46:23 - epoch 129, mean loss per batch 2.850901112721588e-06\n",
      "12-Mar-21 08:46:24 - epoch 130, mean loss per batch 2.806242657872924e-06\n",
      "12-Mar-21 08:46:26 - epoch 131, mean loss per batch 2.7770285074578937e-06\n",
      "12-Mar-21 08:46:27 - epoch 132, mean loss per batch 2.664069195536565e-06\n",
      "12-Mar-21 08:46:29 - epoch 133, mean loss per batch 2.5871395118534006e-06\n",
      "12-Mar-21 08:46:30 - epoch 134, mean loss per batch 2.5555211340570868e-06\n",
      "12-Mar-21 08:46:32 - epoch 135, mean loss per batch 2.5991231408485105e-06\n",
      "12-Mar-21 08:46:33 - epoch 136, mean loss per batch 2.464811044634329e-06\n",
      "12-Mar-21 08:46:34 - epoch 137, mean loss per batch 2.410295070813635e-06\n",
      "12-Mar-21 08:46:36 - epoch 138, mean loss per batch 2.3574498059658185e-06\n",
      "12-Mar-21 08:46:37 - epoch 139, mean loss per batch 2.368274850211422e-06\n",
      "12-Mar-21 08:46:39 - epoch 140, mean loss per batch 2.279180106457856e-06\n",
      "12-Mar-21 08:46:40 - epoch 141, mean loss per batch 2.2606083077264953e-06\n",
      "12-Mar-21 08:46:42 - epoch 142, mean loss per batch 2.2333447960730923e-06\n",
      "12-Mar-21 08:46:43 - epoch 143, mean loss per batch 2.15611579680906e-06\n",
      "12-Mar-21 08:46:44 - epoch 144, mean loss per batch 2.185551400438831e-06\n",
      "12-Mar-21 08:46:46 - epoch 145, mean loss per batch 2.1530583336033697e-06\n",
      "12-Mar-21 08:46:47 - epoch 146, mean loss per batch 2.0928792467272547e-06\n",
      "12-Mar-21 08:46:49 - epoch 147, mean loss per batch 2.069813446653895e-06\n",
      "12-Mar-21 08:46:50 - epoch 148, mean loss per batch 2.0314183888541056e-06\n",
      "12-Mar-21 08:46:52 - epoch 149, mean loss per batch 1.927825070861917e-06\n",
      "12-Mar-21 08:46:53 - epoch 150, mean loss per batch 1.9476857503902397e-06\n",
      "12-Mar-21 08:46:54 - epoch 151, mean loss per batch 1.9130309108452193e-06\n",
      "12-Mar-21 08:46:56 - epoch 152, mean loss per batch 1.8142529256692796e-06\n",
      "12-Mar-21 08:46:57 - epoch 153, mean loss per batch 1.8085773376697034e-06\n",
      "12-Mar-21 08:46:59 - epoch 154, mean loss per batch 1.8013446466424143e-06\n",
      "12-Mar-21 08:47:00 - epoch 155, mean loss per batch 1.739138217369775e-06\n",
      "12-Mar-21 08:47:02 - epoch 156, mean loss per batch 1.6968460491065903e-06\n",
      "12-Mar-21 08:47:03 - epoch 157, mean loss per batch 1.700981836340641e-06\n",
      "12-Mar-21 08:47:05 - epoch 158, mean loss per batch 1.621140628370855e-06\n",
      "12-Mar-21 08:47:06 - epoch 159, mean loss per batch 1.6063619997252176e-06\n",
      "12-Mar-21 08:47:07 - epoch 160, mean loss per batch 1.5996124252873974e-06\n",
      "12-Mar-21 08:47:09 - epoch 161, mean loss per batch 1.559743767442202e-06\n",
      "12-Mar-21 08:47:10 - epoch 162, mean loss per batch 1.5325865748449219e-06\n",
      "12-Mar-21 08:47:12 - epoch 163, mean loss per batch 1.4584122862533359e-06\n",
      "12-Mar-21 08:47:13 - epoch 164, mean loss per batch 1.5004223340698586e-06\n",
      "12-Mar-21 08:47:14 - epoch 165, mean loss per batch 1.4756768022240208e-06\n",
      "12-Mar-21 08:47:16 - epoch 166, mean loss per batch 1.4323037825866218e-06\n",
      "12-Mar-21 08:47:17 - epoch 167, mean loss per batch 1.4164648317861206e-06\n",
      "12-Mar-21 08:47:19 - epoch 168, mean loss per batch 1.389398319745921e-06\n",
      "12-Mar-21 08:47:20 - epoch 169, mean loss per batch 1.3730674525574543e-06\n",
      "12-Mar-21 08:47:22 - epoch 170, mean loss per batch 1.3353066754881672e-06\n",
      "12-Mar-21 08:47:23 - epoch 171, mean loss per batch 1.379699894571856e-06\n",
      "12-Mar-21 08:47:25 - epoch 172, mean loss per batch 1.3108548667052953e-06\n",
      "12-Mar-21 08:47:26 - epoch 173, mean loss per batch 1.2350591523849848e-06\n",
      "12-Mar-21 08:47:27 - epoch 174, mean loss per batch 1.239461377840948e-06\n",
      "12-Mar-21 08:47:29 - epoch 175, mean loss per batch 1.2423279783363866e-06\n",
      "12-Mar-21 08:47:30 - epoch 176, mean loss per batch 1.2016557972261721e-06\n",
      "12-Mar-21 08:47:32 - epoch 177, mean loss per batch 1.2234802791401587e-06\n",
      "12-Mar-21 08:47:33 - epoch 178, mean loss per batch 1.1482611429213507e-06\n",
      "12-Mar-21 08:47:35 - epoch 179, mean loss per batch 1.1363903262498263e-06\n",
      "12-Mar-21 08:47:36 - epoch 180, mean loss per batch 1.1403082042376739e-06\n",
      "12-Mar-21 08:47:38 - epoch 181, mean loss per batch 1.1109453383211915e-06\n",
      "12-Mar-21 08:47:39 - epoch 182, mean loss per batch 1.08459695761317e-06\n",
      "12-Mar-21 08:47:40 - epoch 183, mean loss per batch 1.0949947076929116e-06\n",
      "12-Mar-21 08:47:42 - epoch 184, mean loss per batch 1.0268358733556993e-06\n",
      "12-Mar-21 08:47:43 - epoch 185, mean loss per batch 1.083510660202172e-06\n",
      "12-Mar-21 08:47:44 - epoch 186, mean loss per batch 1.030090065621782e-06\n",
      "12-Mar-21 08:47:46 - epoch 187, mean loss per batch 9.806034619522018e-07\n",
      "12-Mar-21 08:47:47 - epoch 188, mean loss per batch 9.792360650897956e-07\n",
      "12-Mar-21 08:47:49 - epoch 189, mean loss per batch 9.743975882569289e-07\n",
      "12-Mar-21 08:47:50 - epoch 190, mean loss per batch 9.529179819240269e-07\n",
      "12-Mar-21 08:47:52 - epoch 191, mean loss per batch 9.524729198172001e-07\n",
      "12-Mar-21 08:47:53 - epoch 192, mean loss per batch 8.929306770911493e-07\n",
      "12-Mar-21 08:47:55 - epoch 193, mean loss per batch 8.965468422362532e-07\n",
      "12-Mar-21 08:47:56 - epoch 194, mean loss per batch 8.983986718319031e-07\n",
      "12-Mar-21 08:47:57 - epoch 195, mean loss per batch 9.382526072316139e-07\n",
      "12-Mar-21 08:47:59 - epoch 196, mean loss per batch 8.740516079797089e-07\n",
      "12-Mar-21 08:48:00 - epoch 197, mean loss per batch 8.540380393312191e-07\n",
      "12-Mar-21 08:48:02 - epoch 198, mean loss per batch 8.108226421217506e-07\n",
      "12-Mar-21 08:48:03 - epoch 199, mean loss per batch 8.211774677682535e-07\n",
      "12-Mar-21 08:48:05 - epoch 200, mean loss per batch 8.205397359863407e-07\n",
      "12-Mar-21 08:48:06 - epoch 201, mean loss per batch 7.946990675778719e-07\n",
      "12-Mar-21 08:48:08 - epoch 202, mean loss per batch 7.688207557084053e-07\n",
      "12-Mar-21 08:48:09 - epoch 203, mean loss per batch 7.654984262199622e-07\n",
      "12-Mar-21 08:48:10 - epoch 204, mean loss per batch 7.58161525901904e-07\n",
      "12-Mar-21 08:48:12 - epoch 205, mean loss per batch 7.366685023358654e-07\n",
      "12-Mar-21 08:48:13 - epoch 206, mean loss per batch 7.168008266944526e-07\n",
      "12-Mar-21 08:48:15 - epoch 207, mean loss per batch 7.137767316687762e-07\n",
      "12-Mar-21 08:48:16 - epoch 208, mean loss per batch 7.327390239179994e-07\n",
      "12-Mar-21 08:48:18 - epoch 209, mean loss per batch 6.914965327863544e-07\n",
      "12-Mar-21 08:48:19 - epoch 210, mean loss per batch 6.837110220203613e-07\n",
      "12-Mar-21 08:48:21 - epoch 211, mean loss per batch 6.970567045965043e-07\n",
      "12-Mar-21 08:48:22 - epoch 212, mean loss per batch 6.567106276214382e-07\n",
      "12-Mar-21 08:48:23 - epoch 213, mean loss per batch 6.634385687658113e-07\n",
      "12-Mar-21 08:48:25 - epoch 214, mean loss per batch 6.617469698365032e-07\n",
      "12-Mar-21 08:48:26 - epoch 215, mean loss per batch 6.388085941248041e-07\n",
      "12-Mar-21 08:48:28 - epoch 216, mean loss per batch 6.32400341461247e-07\n",
      "12-Mar-21 08:48:29 - epoch 217, mean loss per batch 6.224969826444766e-07\n",
      "12-Mar-21 08:48:30 - epoch 218, mean loss per batch 6.010702002448362e-07\n",
      "12-Mar-21 08:48:32 - epoch 219, mean loss per batch 5.904110240023258e-07\n",
      "12-Mar-21 08:48:33 - epoch 220, mean loss per batch 5.992344147041597e-07\n",
      "12-Mar-21 08:48:35 - epoch 221, mean loss per batch 5.812508874214473e-07\n",
      "12-Mar-21 08:48:36 - epoch 222, mean loss per batch 5.566719614500051e-07\n",
      "12-Mar-21 08:48:38 - epoch 223, mean loss per batch 5.661383386705683e-07\n",
      "12-Mar-21 08:48:39 - epoch 224, mean loss per batch 5.531391976249395e-07\n",
      "12-Mar-21 08:48:40 - epoch 225, mean loss per batch 5.388442343263915e-07\n",
      "12-Mar-21 08:48:42 - epoch 226, mean loss per batch 5.359097048522716e-07\n",
      "12-Mar-21 08:48:43 - epoch 227, mean loss per batch 5.115062457697114e-07\n",
      "12-Mar-21 08:48:45 - epoch 228, mean loss per batch 5.11094345238934e-07\n",
      "12-Mar-21 08:48:46 - epoch 229, mean loss per batch 5.009679782119747e-07\n",
      "12-Mar-21 08:48:48 - epoch 230, mean loss per batch 4.978981728789641e-07\n",
      "12-Mar-21 08:48:49 - epoch 231, mean loss per batch 5.010136600975994e-07\n",
      "12-Mar-21 08:48:51 - epoch 232, mean loss per batch 4.874422818200121e-07\n",
      "12-Mar-21 08:48:52 - epoch 233, mean loss per batch 4.803857496409154e-07\n",
      "12-Mar-21 08:48:54 - epoch 234, mean loss per batch 4.567775124979352e-07\n",
      "12-Mar-21 08:48:55 - epoch 235, mean loss per batch 4.563028819750168e-07\n",
      "12-Mar-21 08:48:56 - epoch 236, mean loss per batch 4.4964570432826946e-07\n",
      "12-Mar-21 08:48:58 - epoch 237, mean loss per batch 4.489857806570049e-07\n",
      "12-Mar-21 08:48:59 - epoch 238, mean loss per batch 4.456553520745169e-07\n",
      "12-Mar-21 08:49:01 - epoch 239, mean loss per batch 4.397530714273811e-07\n",
      "12-Mar-21 08:49:02 - epoch 240, mean loss per batch 4.319586056366321e-07\n",
      "12-Mar-21 08:49:03 - epoch 241, mean loss per batch 4.3210548028582707e-07\n",
      "12-Mar-21 08:49:05 - epoch 242, mean loss per batch 4.2805957670785017e-07\n",
      "12-Mar-21 08:49:06 - epoch 243, mean loss per batch 4.211355356066913e-07\n",
      "12-Mar-21 08:49:08 - epoch 244, mean loss per batch 4.034511435363059e-07\n",
      "12-Mar-21 08:49:09 - epoch 245, mean loss per batch 3.862413240522773e-07\n",
      "12-Mar-21 08:49:10 - epoch 246, mean loss per batch 3.972112632656997e-07\n",
      "12-Mar-21 08:49:12 - epoch 247, mean loss per batch 3.8226708581462913e-07\n",
      "12-Mar-21 08:49:13 - epoch 248, mean loss per batch 3.780528491968373e-07\n",
      "12-Mar-21 08:49:15 - epoch 249, mean loss per batch 3.700918157916861e-07\n",
      "12-Mar-21 08:49:16 - epoch 250, mean loss per batch 3.7879880210311967e-07\n",
      "12-Mar-21 08:49:17 - epoch 251, mean loss per batch 3.5570284440976906e-07\n",
      "12-Mar-21 08:49:19 - epoch 252, mean loss per batch 3.5875832137522735e-07\n",
      "12-Mar-21 08:49:20 - epoch 253, mean loss per batch 3.529339490062109e-07\n",
      "12-Mar-21 08:49:22 - epoch 254, mean loss per batch 3.4828807516672754e-07\n",
      "12-Mar-21 08:49:23 - epoch 255, mean loss per batch 3.356390791964259e-07\n",
      "12-Mar-21 08:49:25 - epoch 256, mean loss per batch 3.316111091705282e-07\n",
      "12-Mar-21 08:49:26 - epoch 257, mean loss per batch 3.2251547019348857e-07\n",
      "12-Mar-21 08:49:28 - epoch 258, mean loss per batch 3.384769437039162e-07\n",
      "12-Mar-21 08:49:29 - epoch 259, mean loss per batch 3.1311626331059347e-07\n",
      "12-Mar-21 08:49:30 - epoch 260, mean loss per batch 3.248581241275763e-07\n",
      "12-Mar-21 08:49:32 - epoch 261, mean loss per batch 3.173690102515208e-07\n",
      "12-Mar-21 08:49:33 - epoch 262, mean loss per batch 3.0858233383941507e-07\n",
      "12-Mar-21 08:49:35 - epoch 263, mean loss per batch 3.076447491970122e-07\n",
      "12-Mar-21 08:49:36 - epoch 264, mean loss per batch 2.976598774641668e-07\n",
      "12-Mar-21 08:49:38 - epoch 265, mean loss per batch 2.9760255579538773e-07\n",
      "12-Mar-21 08:49:39 - epoch 266, mean loss per batch 2.79730980532979e-07\n",
      "12-Mar-21 08:49:41 - epoch 267, mean loss per batch 2.8113513377110604e-07\n",
      "12-Mar-21 08:49:42 - epoch 268, mean loss per batch 2.731024689130488e-07\n",
      "12-Mar-21 08:49:43 - epoch 269, mean loss per batch 2.7170817964780815e-07\n",
      "12-Mar-21 08:49:45 - epoch 270, mean loss per batch 2.706129687514756e-07\n",
      "12-Mar-21 08:49:46 - epoch 271, mean loss per batch 2.7337292498084656e-07\n",
      "12-Mar-21 08:49:48 - epoch 272, mean loss per batch 2.681225716663413e-07\n",
      "12-Mar-21 08:49:49 - epoch 273, mean loss per batch 2.7008552249386554e-07\n",
      "12-Mar-21 08:49:51 - epoch 274, mean loss per batch 2.5002623424632895e-07\n",
      "12-Mar-21 08:49:52 - epoch 275, mean loss per batch 2.455720079935348e-07\n",
      "12-Mar-21 08:49:53 - epoch 276, mean loss per batch 2.6266718414000513e-07\n",
      "12-Mar-21 08:49:55 - epoch 277, mean loss per batch 2.483874707282806e-07\n",
      "12-Mar-21 08:49:56 - epoch 278, mean loss per batch 2.531354315220387e-07\n",
      "12-Mar-21 08:49:58 - epoch 279, mean loss per batch 2.3218243464603242e-07\n",
      "12-Mar-21 08:49:59 - epoch 280, mean loss per batch 2.3037619784487944e-07\n",
      "12-Mar-21 08:50:00 - epoch 281, mean loss per batch 2.3373882002558204e-07\n",
      "12-Mar-21 08:50:02 - epoch 282, mean loss per batch 2.387679677221618e-07\n",
      "12-Mar-21 08:50:03 - epoch 283, mean loss per batch 2.3401105190364783e-07\n",
      "12-Mar-21 08:50:05 - epoch 284, mean loss per batch 2.219692218064805e-07\n",
      "12-Mar-21 08:50:06 - epoch 285, mean loss per batch 2.2435932031874893e-07\n",
      "12-Mar-21 08:50:07 - epoch 286, mean loss per batch 2.1385953739529492e-07\n",
      "12-Mar-21 08:50:09 - epoch 287, mean loss per batch 2.0636328119831677e-07\n",
      "12-Mar-21 08:50:10 - epoch 288, mean loss per batch 2.1797884987615993e-07\n",
      "12-Mar-21 08:50:12 - epoch 289, mean loss per batch 2.079268269356432e-07\n",
      "12-Mar-21 08:50:13 - epoch 290, mean loss per batch 2.0277320964204102e-07\n",
      "12-Mar-21 08:50:15 - epoch 291, mean loss per batch 1.9765271730420864e-07\n",
      "12-Mar-21 08:50:16 - epoch 292, mean loss per batch 1.9599245986840358e-07\n",
      "12-Mar-21 08:50:17 - epoch 293, mean loss per batch 1.9170030458763694e-07\n",
      "12-Mar-21 08:50:19 - epoch 294, mean loss per batch 1.8991288569098359e-07\n",
      "12-Mar-21 08:50:20 - epoch 295, mean loss per batch 1.9484621396678938e-07\n",
      "12-Mar-21 08:50:22 - epoch 296, mean loss per batch 1.8215692803708478e-07\n",
      "12-Mar-21 08:50:23 - epoch 297, mean loss per batch 1.8100800010992876e-07\n",
      "12-Mar-21 08:50:25 - epoch 298, mean loss per batch 1.7803761336993446e-07\n",
      "12-Mar-21 08:50:26 - epoch 299, mean loss per batch 1.7357263195969188e-07\n",
      "12-Mar-21 08:50:27 - epoch 300, mean loss per batch 1.7355562457274013e-07\n",
      "12-Mar-21 08:50:29 - epoch 301, mean loss per batch 1.703703164187468e-07\n",
      "12-Mar-21 08:50:30 - epoch 302, mean loss per batch 1.6908795867738473e-07\n",
      "12-Mar-21 08:50:32 - epoch 303, mean loss per batch 1.7070792287435884e-07\n",
      "12-Mar-21 08:50:33 - epoch 304, mean loss per batch 1.636665771798107e-07\n",
      "12-Mar-21 08:50:35 - epoch 305, mean loss per batch 1.599162121014895e-07\n",
      "12-Mar-21 08:50:36 - epoch 306, mean loss per batch 1.5912369213730506e-07\n",
      "12-Mar-21 08:50:37 - epoch 307, mean loss per batch 1.6080634615342694e-07\n",
      "12-Mar-21 08:50:39 - epoch 308, mean loss per batch 1.5097998256736062e-07\n",
      "12-Mar-21 08:50:40 - epoch 309, mean loss per batch 1.5698165065294654e-07\n",
      "12-Mar-21 08:50:42 - epoch 310, mean loss per batch 1.4985881527160507e-07\n",
      "12-Mar-21 08:50:43 - epoch 311, mean loss per batch 1.5084924270398076e-07\n",
      "12-Mar-21 08:50:45 - epoch 312, mean loss per batch 1.438723765764175e-07\n",
      "12-Mar-21 08:50:46 - epoch 313, mean loss per batch 1.4969225175648788e-07\n",
      "12-Mar-21 08:50:47 - epoch 314, mean loss per batch 1.3947188211187284e-07\n",
      "12-Mar-21 08:50:49 - epoch 315, mean loss per batch 1.419264470734141e-07\n",
      "12-Mar-21 08:50:50 - epoch 316, mean loss per batch 1.377945997988333e-07\n",
      "12-Mar-21 08:50:52 - epoch 317, mean loss per batch 1.304075917337636e-07\n",
      "12-Mar-21 08:50:53 - epoch 318, mean loss per batch 1.3042370957586732e-07\n",
      "12-Mar-21 08:50:55 - epoch 319, mean loss per batch 1.3106578823960893e-07\n",
      "12-Mar-21 08:50:56 - epoch 320, mean loss per batch 1.2718557147632754e-07\n",
      "12-Mar-21 08:50:58 - epoch 321, mean loss per batch 1.3096369964562024e-07\n",
      "12-Mar-21 08:50:59 - epoch 322, mean loss per batch 1.2401280713447704e-07\n",
      "12-Mar-21 08:51:01 - epoch 323, mean loss per batch 1.3053117725165137e-07\n",
      "12-Mar-21 08:51:02 - epoch 324, mean loss per batch 1.1996602603622145e-07\n",
      "12-Mar-21 08:51:03 - epoch 325, mean loss per batch 1.2083376460098478e-07\n",
      "12-Mar-21 08:51:05 - epoch 326, mean loss per batch 1.163356565952584e-07\n",
      "12-Mar-21 08:51:06 - epoch 327, mean loss per batch 1.219764312802104e-07\n",
      "12-Mar-21 08:51:08 - epoch 328, mean loss per batch 1.1563000020723499e-07\n",
      "12-Mar-21 08:51:09 - epoch 329, mean loss per batch 1.1552164353317424e-07\n",
      "12-Mar-21 08:51:11 - epoch 330, mean loss per batch 1.0971341496839999e-07\n",
      "12-Mar-21 08:51:12 - epoch 331, mean loss per batch 1.1097517735528037e-07\n",
      "12-Mar-21 08:51:14 - epoch 332, mean loss per batch 1.084910529960745e-07\n",
      "12-Mar-21 08:51:15 - epoch 333, mean loss per batch 1.0562185819979347e-07\n",
      "12-Mar-21 08:51:17 - epoch 334, mean loss per batch 1.0310818127506467e-07\n",
      "12-Mar-21 08:51:18 - epoch 335, mean loss per batch 1.0685317710987455e-07\n",
      "12-Mar-21 08:51:20 - epoch 336, mean loss per batch 1.0000795268839423e-07\n",
      "12-Mar-21 08:51:21 - epoch 337, mean loss per batch 1.0113449332694611e-07\n",
      "12-Mar-21 08:51:22 - epoch 338, mean loss per batch 1.0084256055838873e-07\n",
      "12-Mar-21 08:51:24 - epoch 339, mean loss per batch 9.975810864550392e-08\n",
      "12-Mar-21 08:51:25 - epoch 340, mean loss per batch 9.506925448586291e-08\n",
      "12-Mar-21 08:51:27 - epoch 341, mean loss per batch 9.487403327914133e-08\n",
      "12-Mar-21 08:51:28 - epoch 342, mean loss per batch 9.100546149144205e-08\n",
      "12-Mar-21 08:51:30 - epoch 343, mean loss per batch 9.21302134752995e-08\n",
      "12-Mar-21 08:51:31 - epoch 344, mean loss per batch 9.066516809114616e-08\n",
      "12-Mar-21 08:51:33 - epoch 345, mean loss per batch 9.215260049061796e-08\n",
      "12-Mar-21 08:51:34 - epoch 346, mean loss per batch 9.271050224959673e-08\n",
      "12-Mar-21 08:51:36 - epoch 347, mean loss per batch 8.87998390033737e-08\n",
      "12-Mar-21 08:51:37 - epoch 348, mean loss per batch 8.699808490702638e-08\n",
      "12-Mar-21 08:51:38 - epoch 349, mean loss per batch 8.472709057426843e-08\n",
      "12-Mar-21 08:51:40 - epoch 350, mean loss per batch 8.545245031235446e-08\n",
      "12-Mar-21 08:51:41 - epoch 351, mean loss per batch 8.061136004864868e-08\n",
      "12-Mar-21 08:51:43 - epoch 352, mean loss per batch 8.035972284185041e-08\n",
      "12-Mar-21 08:51:44 - epoch 353, mean loss per batch 7.944362323542009e-08\n",
      "12-Mar-21 08:51:46 - epoch 354, mean loss per batch 7.788723949558447e-08\n",
      "12-Mar-21 08:51:47 - epoch 355, mean loss per batch 7.697293277977643e-08\n",
      "12-Mar-21 08:51:48 - epoch 356, mean loss per batch 7.724874797951578e-08\n",
      "12-Mar-21 08:51:50 - epoch 357, mean loss per batch 7.471716100527513e-08\n",
      "12-Mar-21 08:51:51 - epoch 358, mean loss per batch 7.42291113362206e-08\n",
      "12-Mar-21 08:51:53 - epoch 359, mean loss per batch 7.510133082943273e-08\n",
      "12-Mar-21 08:51:54 - epoch 360, mean loss per batch 7.25822831581214e-08\n",
      "12-Mar-21 08:51:56 - epoch 361, mean loss per batch 7.445119761720994e-08\n",
      "12-Mar-21 08:51:57 - epoch 362, mean loss per batch 6.875400734210976e-08\n",
      "12-Mar-21 08:51:58 - epoch 363, mean loss per batch 6.749582663718894e-08\n",
      "12-Mar-21 08:52:00 - epoch 364, mean loss per batch 6.789701218351114e-08\n",
      "12-Mar-21 08:52:01 - epoch 365, mean loss per batch 6.828745131752629e-08\n",
      "12-Mar-21 08:52:03 - epoch 366, mean loss per batch 6.363442005808705e-08\n",
      "12-Mar-21 08:52:04 - epoch 367, mean loss per batch 6.406784231343794e-08\n",
      "12-Mar-21 08:52:06 - epoch 368, mean loss per batch 6.202699222622134e-08\n",
      "12-Mar-21 08:52:07 - epoch 369, mean loss per batch 6.143596004575942e-08\n",
      "12-Mar-21 08:52:09 - epoch 370, mean loss per batch 6.206639414384742e-08\n",
      "12-Mar-21 08:52:10 - epoch 371, mean loss per batch 6.228489628330618e-08\n",
      "12-Mar-21 08:52:11 - epoch 372, mean loss per batch 5.9120189359436367e-08\n",
      "12-Mar-21 08:52:13 - epoch 373, mean loss per batch 5.853900871736292e-08\n",
      "12-Mar-21 08:52:14 - epoch 374, mean loss per batch 5.68796420561914e-08\n",
      "12-Mar-21 08:52:16 - epoch 375, mean loss per batch 5.80921529000569e-08\n",
      "12-Mar-21 08:52:17 - epoch 376, mean loss per batch 5.750828408351411e-08\n",
      "12-Mar-21 08:52:19 - epoch 377, mean loss per batch 5.535997356258367e-08\n",
      "12-Mar-21 08:52:20 - epoch 378, mean loss per batch 5.5881156932549366e-08\n",
      "12-Mar-21 08:52:21 - epoch 379, mean loss per batch 5.738918400328867e-08\n",
      "12-Mar-21 08:52:23 - epoch 380, mean loss per batch 5.414298534298303e-08\n",
      "12-Mar-21 08:52:24 - epoch 381, mean loss per batch 5.036306838235305e-08\n",
      "12-Mar-21 08:52:25 - epoch 382, mean loss per batch 5.18657217003675e-08\n",
      "12-Mar-21 08:52:27 - epoch 383, mean loss per batch 5.271913640679206e-08\n",
      "12-Mar-21 08:52:28 - epoch 384, mean loss per batch 5.1748411641264845e-08\n",
      "12-Mar-21 08:52:30 - epoch 385, mean loss per batch 4.8510272926832315e-08\n",
      "12-Mar-21 08:52:31 - epoch 386, mean loss per batch 4.8154758266386686e-08\n",
      "12-Mar-21 08:52:33 - epoch 387, mean loss per batch 4.61658470084759e-08\n",
      "12-Mar-21 08:52:34 - epoch 388, mean loss per batch 4.7577159442915637e-08\n",
      "12-Mar-21 08:52:36 - epoch 389, mean loss per batch 4.53187015435875e-08\n",
      "12-Mar-21 08:52:37 - epoch 390, mean loss per batch 4.5863167349361554e-08\n",
      "12-Mar-21 08:52:38 - epoch 391, mean loss per batch 4.561959015675781e-08\n",
      "12-Mar-21 08:52:40 - epoch 392, mean loss per batch 4.437663255666499e-08\n",
      "12-Mar-21 08:52:41 - epoch 393, mean loss per batch 4.4301410266729576e-08\n",
      "12-Mar-21 08:52:43 - epoch 394, mean loss per batch 4.259368673149748e-08\n",
      "12-Mar-21 08:52:44 - epoch 395, mean loss per batch 4.321427017937542e-08\n",
      "12-Mar-21 08:52:46 - epoch 396, mean loss per batch 4.12504327448241e-08\n",
      "12-Mar-21 08:52:47 - epoch 397, mean loss per batch 4.2783533433958624e-08\n",
      "12-Mar-21 08:52:48 - epoch 398, mean loss per batch 4.021165055635178e-08\n",
      "12-Mar-21 08:52:50 - epoch 399, mean loss per batch 4.050985229315966e-08\n",
      "12-Mar-21 08:52:51 - epoch 400, mean loss per batch 3.8426913480275985e-08\n",
      "12-Mar-21 08:52:53 - epoch 401, mean loss per batch 4.035134879223698e-08\n",
      "12-Mar-21 08:52:54 - epoch 402, mean loss per batch 3.8373183126776226e-08\n",
      "12-Mar-21 08:52:56 - epoch 403, mean loss per batch 3.9114659818787565e-08\n",
      "12-Mar-21 08:52:57 - epoch 404, mean loss per batch 3.7632602743433204e-08\n",
      "12-Mar-21 08:52:58 - epoch 405, mean loss per batch 3.733350517620693e-08\n",
      "12-Mar-21 08:53:00 - epoch 406, mean loss per batch 3.7920954854852283e-08\n",
      "12-Mar-21 08:53:01 - epoch 407, mean loss per batch 3.576100236448208e-08\n",
      "12-Mar-21 08:53:03 - epoch 408, mean loss per batch 3.3634183438380205e-08\n",
      "12-Mar-21 08:53:04 - epoch 409, mean loss per batch 3.435148104430779e-08\n",
      "12-Mar-21 08:53:06 - epoch 410, mean loss per batch 3.298852609882138e-08\n",
      "12-Mar-21 08:53:07 - epoch 411, mean loss per batch 3.282285759426556e-08\n",
      "12-Mar-21 08:53:09 - epoch 412, mean loss per batch 3.6044877373451705e-08\n",
      "12-Mar-21 08:53:10 - epoch 413, mean loss per batch 3.2606146500750825e-08\n",
      "12-Mar-21 08:53:12 - epoch 414, mean loss per batch 3.263480268928403e-08\n",
      "12-Mar-21 08:53:13 - epoch 415, mean loss per batch 3.204914385161846e-08\n",
      "12-Mar-21 08:53:15 - epoch 416, mean loss per batch 3.107931450658113e-08\n",
      "12-Mar-21 08:53:16 - epoch 417, mean loss per batch 3.020261709106255e-08\n",
      "12-Mar-21 08:53:17 - epoch 418, mean loss per batch 3.143841016577216e-08\n",
      "12-Mar-21 08:53:19 - epoch 419, mean loss per batch 3.08169319806185e-08\n",
      "12-Mar-21 08:53:20 - epoch 420, mean loss per batch 2.9755760727185205e-08\n",
      "12-Mar-21 08:53:21 - epoch 421, mean loss per batch 2.8905929137469965e-08\n",
      "12-Mar-21 08:53:23 - epoch 422, mean loss per batch 2.837399969680431e-08\n",
      "12-Mar-21 08:53:24 - epoch 423, mean loss per batch 2.8595189395839672e-08\n",
      "12-Mar-21 08:53:26 - epoch 424, mean loss per batch 2.7317307253855898e-08\n",
      "12-Mar-21 08:53:27 - epoch 425, mean loss per batch 2.5983007472081436e-08\n",
      "12-Mar-21 08:53:29 - epoch 426, mean loss per batch 2.7479392914988065e-08\n",
      "12-Mar-21 08:53:30 - epoch 427, mean loss per batch 2.6385088165607858e-08\n",
      "12-Mar-21 08:53:32 - epoch 428, mean loss per batch 2.6023305630054402e-08\n",
      "12-Mar-21 08:53:33 - epoch 429, mean loss per batch 2.5581822267366324e-08\n",
      "12-Mar-21 08:53:34 - epoch 430, mean loss per batch 2.551376450281413e-08\n",
      "12-Mar-21 08:53:36 - epoch 431, mean loss per batch 2.5209293182863e-08\n",
      "12-Mar-21 08:53:37 - epoch 432, mean loss per batch 2.408364646172983e-08\n",
      "12-Mar-21 08:53:39 - epoch 433, mean loss per batch 2.4700647886041118e-08\n",
      "12-Mar-21 08:53:40 - epoch 434, mean loss per batch 2.342903430237934e-08\n",
      "12-Mar-21 08:53:42 - epoch 435, mean loss per batch 2.4344238010069848e-08\n",
      "12-Mar-21 08:53:43 - epoch 436, mean loss per batch 2.3412915367132953e-08\n",
      "12-Mar-21 08:53:44 - epoch 437, mean loss per batch 2.323829238439255e-08\n",
      "12-Mar-21 08:53:46 - epoch 438, mean loss per batch 2.2058915070634665e-08\n",
      "12-Mar-21 08:53:48 - epoch 439, mean loss per batch 2.183056200768017e-08\n",
      "12-Mar-21 08:53:49 - epoch 440, mean loss per batch 2.1163412150953647e-08\n",
      "12-Mar-21 08:53:50 - epoch 441, mean loss per batch 2.0095077260315556e-08\n",
      "12-Mar-21 08:53:52 - epoch 442, mean loss per batch 2.174280219117227e-08\n",
      "12-Mar-21 08:53:53 - epoch 443, mean loss per batch 2.1534150257834355e-08\n",
      "12-Mar-21 08:53:55 - epoch 444, mean loss per batch 2.049536742030857e-08\n",
      "12-Mar-21 08:53:56 - epoch 445, mean loss per batch 2.096729689017051e-08\n",
      "12-Mar-21 08:53:58 - epoch 446, mean loss per batch 1.9942841839464947e-08\n",
      "12-Mar-21 08:53:59 - epoch 447, mean loss per batch 1.8706152558569155e-08\n",
      "12-Mar-21 08:54:01 - epoch 448, mean loss per batch 1.9353600875750597e-08\n",
      "12-Mar-21 08:54:02 - epoch 449, mean loss per batch 1.9046444045865375e-08\n",
      "12-Mar-21 08:54:03 - epoch 450, mean loss per batch 1.947359842610843e-08\n",
      "12-Mar-21 08:54:05 - epoch 451, mean loss per batch 1.7979900064528483e-08\n",
      "12-Mar-21 08:54:06 - epoch 452, mean loss per batch 1.850376918355323e-08\n",
      "12-Mar-21 08:54:08 - epoch 453, mean loss per batch 1.78339333386427e-08\n",
      "12-Mar-21 08:54:09 - epoch 454, mean loss per batch 1.7225886699962373e-08\n",
      "12-Mar-21 08:54:10 - epoch 455, mean loss per batch 1.7775725199479318e-08\n",
      "12-Mar-21 08:54:12 - epoch 456, mean loss per batch 1.7727368052133077e-08\n",
      "12-Mar-21 08:54:13 - epoch 457, mean loss per batch 1.7028876155332174e-08\n",
      "12-Mar-21 08:54:15 - epoch 458, mean loss per batch 1.6690376431354864e-08\n",
      "12-Mar-21 08:54:16 - epoch 459, mean loss per batch 1.5992779305990396e-08\n",
      "12-Mar-21 08:54:18 - epoch 460, mean loss per batch 1.5308615537616806e-08\n",
      "12-Mar-21 08:54:19 - epoch 461, mean loss per batch 1.4851013752092967e-08\n",
      "12-Mar-21 08:54:21 - epoch 462, mean loss per batch 1.4735493850755924e-08\n",
      "12-Mar-21 08:54:22 - epoch 463, mean loss per batch 1.52727957118788e-08\n",
      "12-Mar-21 08:54:23 - epoch 464, mean loss per batch 1.5003249241264992e-08\n",
      "12-Mar-21 08:54:25 - epoch 465, mean loss per batch 1.4074612825637063e-08\n",
      "12-Mar-21 08:54:26 - epoch 466, mean loss per batch 1.391521366509953e-08\n",
      "12-Mar-21 08:54:28 - epoch 467, mean loss per batch 1.35668630517872e-08\n",
      "12-Mar-21 08:54:29 - epoch 468, mean loss per batch 1.3788051965126268e-08\n",
      "12-Mar-21 08:54:31 - epoch 469, mean loss per batch 1.3413731971321742e-08\n",
      "12-Mar-21 08:54:32 - epoch 470, mean loss per batch 1.3347465013588625e-08\n",
      "12-Mar-21 08:54:33 - epoch 471, mean loss per batch 1.2851356474723293e-08\n",
      "12-Mar-21 08:54:35 - epoch 472, mean loss per batch 1.2835237693200096e-08\n",
      "12-Mar-21 08:54:36 - epoch 473, mean loss per batch 1.2626585213290845e-08\n",
      "12-Mar-21 08:54:38 - epoch 474, mean loss per batch 1.2666883132138851e-08\n",
      "12-Mar-21 08:54:39 - epoch 475, mean loss per batch 1.2517334109400753e-08\n",
      "12-Mar-21 08:54:40 - epoch 476, mean loss per batch 1.2321218848617614e-08\n",
      "12-Mar-21 08:54:42 - epoch 477, mean loss per batch 1.22334597836453e-08\n",
      "12-Mar-21 08:54:43 - epoch 478, mean loss per batch 1.1790186024066648e-08\n",
      "12-Mar-21 08:54:45 - epoch 479, mean loss per batch 1.1420343474318665e-08\n",
      "12-Mar-21 08:54:46 - epoch 480, mean loss per batch 1.1306614721407781e-08\n",
      "12-Mar-21 08:54:48 - epoch 481, mean loss per batch 1.1817946518824293e-08\n",
      "12-Mar-21 08:54:49 - epoch 482, mean loss per batch 1.1428403044423984e-08\n",
      "12-Mar-21 08:54:51 - epoch 483, mean loss per batch 1.0422753600362984e-08\n",
      "12-Mar-21 08:54:52 - epoch 484, mean loss per batch 1.0837371342211974e-08\n",
      "12-Mar-21 08:54:53 - epoch 485, mean loss per batch 9.922167532039339e-09\n",
      "12-Mar-21 08:54:55 - epoch 486, mean loss per batch 1.131109223378574e-08\n",
      "12-Mar-21 08:54:56 - epoch 487, mean loss per batch 1.009052210164076e-08\n",
      "12-Mar-21 08:54:58 - epoch 488, mean loss per batch 1.0043060400404942e-08\n",
      "12-Mar-21 08:54:59 - epoch 489, mean loss per batch 9.365165096411396e-09\n",
      "12-Mar-21 08:55:00 - epoch 490, mean loss per batch 9.201288005027227e-09\n",
      "12-Mar-21 08:55:02 - epoch 491, mean loss per batch 9.768141270208949e-09\n",
      "12-Mar-21 08:55:03 - epoch 492, mean loss per batch 8.683687632208366e-09\n",
      "12-Mar-21 08:55:05 - epoch 493, mean loss per batch 8.732044796634963e-09\n",
      "12-Mar-21 08:55:06 - epoch 494, mean loss per batch 8.692642725285701e-09\n",
      "12-Mar-21 08:55:08 - epoch 495, mean loss per batch 8.920100367750305e-09\n",
      "12-Mar-21 08:55:09 - epoch 496, mean loss per batch 8.611151894108652e-09\n",
      "12-Mar-21 08:55:11 - epoch 497, mean loss per batch 8.362202204186094e-09\n",
      "12-Mar-21 08:55:12 - epoch 498, mean loss per batch 8.687269672855372e-09\n",
      "12-Mar-21 08:55:13 - epoch 499, mean loss per batch 7.840124259208036e-09\n",
      "12-Mar-21 08:55:15 - epoch 500, mean loss per batch 8.142804140366987e-09\n",
      "12-Mar-21 08:55:16 - epoch 501, mean loss per batch 7.625203665066001e-09\n",
      "12-Mar-21 08:55:18 - epoch 502, mean loss per batch 8.236831968625543e-09\n",
      "12-Mar-21 08:55:19 - epoch 503, mean loss per batch 8.388171764022008e-09\n",
      "12-Mar-21 08:55:20 - epoch 504, mean loss per batch 7.4353571590289284e-09\n",
      "12-Mar-21 08:55:22 - epoch 505, mean loss per batch 6.729701138233133e-09\n",
      "12-Mar-21 08:55:23 - epoch 506, mean loss per batch 6.822833464870115e-09\n",
      "12-Mar-21 08:55:25 - epoch 507, mean loss per batch 6.497766051711988e-09\n",
      "12-Mar-21 08:55:26 - epoch 508, mean loss per batch 7.164915430188592e-09\n",
      "12-Mar-21 08:55:28 - epoch 509, mean loss per batch 6.6437329185106914e-09\n",
      "12-Mar-21 08:55:29 - epoch 510, mean loss per batch 7.1299908712172915e-09\n",
      "12-Mar-21 08:55:30 - epoch 511, mean loss per batch 6.105535948614388e-09\n",
      "12-Mar-21 08:55:32 - epoch 512, mean loss per batch 6.617763362944865e-09\n",
      "12-Mar-21 08:55:33 - epoch 513, mean loss per batch 6.301650995893099e-09\n",
      "12-Mar-21 08:55:35 - epoch 514, mean loss per batch 5.895988351391533e-09\n",
      "12-Mar-21 08:55:36 - epoch 515, mean loss per batch 5.833303143939398e-09\n",
      "12-Mar-21 08:55:38 - epoch 516, mean loss per batch 6.391201260532634e-09\n",
      "12-Mar-21 08:55:39 - epoch 517, mean loss per batch 5.660471237033649e-09\n",
      "12-Mar-21 08:55:40 - epoch 518, mean loss per batch 5.958673605814642e-09\n",
      "12-Mar-21 08:55:42 - epoch 519, mean loss per batch 5.416894526949917e-09\n",
      "12-Mar-21 08:55:43 - epoch 520, mean loss per batch 5.907629872471996e-09\n",
      "12-Mar-21 08:55:45 - epoch 521, mean loss per batch 5.7133058625971405e-09\n",
      "12-Mar-21 08:55:46 - epoch 522, mean loss per batch 5.0121274310408985e-09\n",
      "12-Mar-21 08:55:47 - epoch 523, mean loss per batch 5.031828445365085e-09\n",
      "12-Mar-21 08:55:49 - epoch 524, mean loss per batch 4.974516303153546e-09\n",
      "12-Mar-21 08:55:50 - epoch 525, mean loss per batch 4.953919718886368e-09\n",
      "12-Mar-21 08:55:52 - epoch 526, mean loss per batch 5.412417078623286e-09\n",
      "12-Mar-21 08:55:53 - epoch 527, mean loss per batch 4.957501793694082e-09\n",
      "12-Mar-21 08:55:55 - epoch 528, mean loss per batch 4.761386703714484e-09\n",
      "12-Mar-21 08:55:56 - epoch 529, mean loss per batch 4.727357595123694e-09\n",
      "12-Mar-21 08:55:57 - epoch 530, mean loss per batch 4.91720418229245e-09\n",
      "12-Mar-21 08:55:59 - epoch 531, mean loss per batch 4.459602362607904e-09\n",
      "12-Mar-21 08:56:00 - epoch 532, mean loss per batch 4.7649687785221975e-09\n",
      "12-Mar-21 08:56:02 - epoch 533, mean loss per batch 4.171250533014669e-09\n",
      "12-Mar-21 08:56:03 - epoch 534, mean loss per batch 4.533033615139459e-09\n",
      "12-Mar-21 08:56:05 - epoch 535, mean loss per batch 4.469452944496547e-09\n",
      "12-Mar-21 08:56:06 - epoch 536, mean loss per batch 3.848869684502504e-09\n",
      "12-Mar-21 08:56:08 - epoch 537, mean loss per batch 3.7279768075967246e-09\n",
      "12-Mar-21 08:56:09 - epoch 538, mean loss per batch 3.688574740517553e-09\n",
      "12-Mar-21 08:56:10 - epoch 539, mean loss per batch 3.8837942605541596e-09\n",
      "12-Mar-21 08:56:12 - epoch 540, mean loss per batch 4.184683121389609e-09\n",
      "12-Mar-21 08:56:13 - epoch 541, mean loss per batch 3.525593278857614e-09\n",
      "12-Mar-21 08:56:15 - epoch 542, mean loss per batch 3.2775391033668964e-09\n",
      "12-Mar-21 08:56:16 - epoch 543, mean loss per batch 3.5703684794987987e-09\n",
      "12-Mar-21 08:56:17 - epoch 544, mean loss per batch 3.334851279739144e-09\n",
      "12-Mar-21 08:56:19 - epoch 545, mean loss per batch 3.603502030956863e-09\n",
      "12-Mar-21 08:56:20 - epoch 546, mean loss per batch 3.319627718011676e-09\n",
      "12-Mar-21 08:56:22 - epoch 547, mean loss per batch 3.1996303811582676e-09\n",
      "12-Mar-21 08:56:23 - epoch 548, mean loss per batch 3.426192517731207e-09\n",
      "12-Mar-21 08:56:25 - epoch 549, mean loss per batch 3.192466359645498e-09\n",
      "12-Mar-21 08:56:26 - epoch 550, mean loss per batch 2.955158177993138e-09\n",
      "12-Mar-21 08:56:27 - epoch 551, mean loss per batch 2.996351303826609e-09\n",
      "12-Mar-21 08:56:29 - epoch 552, mean loss per batch 3.075155557547432e-09\n",
      "12-Mar-21 08:56:30 - epoch 553, mean loss per batch 2.8969505256198476e-09\n",
      "12-Mar-21 08:56:32 - epoch 554, mean loss per batch 2.9408301563180408e-09\n",
      "12-Mar-21 08:56:33 - epoch 555, mean loss per batch 2.6954624686116912e-09\n",
      "12-Mar-21 08:56:35 - epoch 556, mean loss per batch 2.983814298204926e-09\n",
      "12-Mar-21 08:56:36 - epoch 557, mean loss per batch 2.373081568858464e-09\n",
      "12-Mar-21 08:56:37 - epoch 558, mean loss per batch 2.448303769122016e-09\n",
      "12-Mar-21 08:56:39 - epoch 559, mean loss per batch 2.76531165487571e-09\n",
      "12-Mar-21 08:56:40 - epoch 560, mean loss per batch 2.283531312759106e-09\n",
      "12-Mar-21 08:56:42 - epoch 561, mean loss per batch 2.2387561932496043e-09\n",
      "12-Mar-21 08:56:43 - epoch 562, mean loss per batch 2.1850260575243614e-09\n",
      "12-Mar-21 08:56:44 - epoch 563, mean loss per batch 2.499347472574042e-09\n",
      "12-Mar-21 08:56:46 - epoch 564, mean loss per batch 2.1671160080125254e-09\n",
      "12-Mar-21 08:56:47 - epoch 565, mean loss per batch 2.456363305066626e-09\n",
      "12-Mar-21 08:56:49 - epoch 566, mean loss per batch 2.1581609747164303e-09\n",
      "12-Mar-21 08:56:50 - epoch 567, mean loss per batch 2.183235050011125e-09\n",
      "12-Mar-21 08:56:52 - epoch 568, mean loss per batch 2.101744338396554e-09\n",
      "12-Mar-21 08:56:53 - epoch 569, mean loss per batch 2.167116029362968e-09\n",
      "12-Mar-21 08:56:54 - epoch 570, mean loss per batch 2.3721861227480408e-09\n",
      "12-Mar-21 08:56:56 - epoch 571, mean loss per batch 2.0095075688922965e-09\n",
      "12-Mar-21 08:56:57 - epoch 572, mean loss per batch 2.0417456614296725e-09\n",
      "12-Mar-21 08:56:59 - epoch 573, mean loss per batch 2.049805167483662e-09\n",
      "12-Mar-21 08:57:00 - epoch 574, mean loss per batch 1.956672919843318e-09\n",
      "12-Mar-21 08:57:02 - epoch 575, mean loss per batch 1.87339121566082e-09\n",
      "12-Mar-21 08:57:03 - epoch 576, mean loss per batch 1.7910049810742323e-09\n",
      "12-Mar-21 08:57:05 - epoch 577, mean loss per batch 1.7632443838344614e-09\n",
      "12-Mar-21 08:57:06 - epoch 578, mean loss per batch 1.6790671545449025e-09\n",
      "12-Mar-21 08:57:08 - epoch 579, mean loss per batch 1.8026464914794742e-09\n",
      "12-Mar-21 08:57:09 - epoch 580, mean loss per batch 1.5984719274715513e-09\n",
      "12-Mar-21 08:57:11 - epoch 581, mean loss per batch 1.315493143903514e-09\n",
      "12-Mar-21 08:57:12 - epoch 582, mean loss per batch 1.4820566205899264e-09\n",
      "12-Mar-21 08:57:14 - epoch 583, mean loss per batch 1.428326448568931e-09\n",
      "12-Mar-21 08:57:15 - epoch 584, mean loss per batch 1.4238489361909719e-09\n",
      "12-Mar-21 08:57:17 - epoch 585, mean loss per batch 1.565338376013487e-09\n",
      "12-Mar-21 08:57:18 - epoch 586, mean loss per batch 1.3799693353833986e-09\n",
      "12-Mar-21 08:57:20 - epoch 587, mean loss per batch 1.208928354859204e-09\n",
      "12-Mar-21 08:57:21 - epoch 588, mean loss per batch 1.2492259577206584e-09\n",
      "12-Mar-21 08:57:23 - epoch 589, mean loss per batch 1.3342987142523226e-09\n",
      "12-Mar-21 08:57:24 - epoch 590, mean loss per batch 1.1059455658960578e-09\n",
      "12-Mar-21 08:57:26 - epoch 591, mean loss per batch 1.1551982063236958e-09\n",
      "12-Mar-21 08:57:27 - epoch 592, mean loss per batch 1.3110156422007764e-09\n",
      "12-Mar-21 08:57:29 - epoch 593, mean loss per batch 1.1632577337281282e-09\n",
      "12-Mar-21 08:57:30 - epoch 594, mean loss per batch 1.1229601137863199e-09\n",
      "12-Mar-21 08:57:32 - epoch 595, mean loss per batch 1.0343053721188018e-09\n",
      "12-Mar-21 08:57:33 - epoch 596, mean loss per batch 1.1238556282181595e-09\n",
      "12-Mar-21 08:57:35 - epoch 597, mean loss per batch 1.12564664747414e-09\n",
      "12-Mar-21 08:57:36 - epoch 598, mean loss per batch 1.1238556282181595e-09\n",
      "12-Mar-21 08:57:38 - epoch 599, mean loss per batch 1.0611704506566446e-09\n",
      "12-Mar-21 08:57:39 - epoch 600, mean loss per batch 1.308329167226674e-09\n",
      "12-Mar-21 08:57:41 - epoch 601, mean loss per batch 1.1865408089822411e-09\n",
      "12-Mar-21 08:57:42 - epoch 602, mean loss per batch 9.895302483392117e-10\n",
      "12-Mar-21 08:57:43 - epoch 603, mean loss per batch 1.1301241651897096e-09\n",
      "12-Mar-21 08:57:45 - epoch 604, mean loss per batch 9.268450771828294e-10\n",
      "12-Mar-21 08:57:46 - epoch 605, mean loss per batch 8.498318544607304e-10\n",
      "12-Mar-21 08:57:48 - epoch 606, mean loss per batch 9.358001126139688e-10\n",
      "12-Mar-21 08:57:49 - epoch 607, mean loss per batch 8.283398091378193e-10\n",
      "12-Mar-21 08:57:51 - epoch 608, mean loss per batch 8.32817310840557e-10\n",
      "12-Mar-21 08:57:52 - epoch 609, mean loss per batch 1.0387828951719824e-09\n",
      "12-Mar-21 08:57:54 - epoch 610, mean loss per batch 7.477445820644681e-10\n",
      "12-Mar-21 08:57:55 - epoch 611, mean loss per batch 1.01012681766108e-09\n",
      "12-Mar-21 08:57:57 - epoch 612, mean loss per batch 7.746096531296559e-10\n",
      "12-Mar-21 08:57:58 - epoch 613, mean loss per batch 7.253570255122838e-10\n",
      "12-Mar-21 08:58:00 - epoch 614, mean loss per batch 8.668464322416123e-10\n",
      "12-Mar-21 08:58:01 - epoch 615, mean loss per batch 6.940144313939155e-10\n",
      "12-Mar-21 08:58:03 - epoch 616, mean loss per batch 6.841639071514677e-10\n",
      "12-Mar-21 08:58:04 - epoch 617, mean loss per batch 5.731216035941545e-10\n",
      "12-Mar-21 08:58:06 - epoch 618, mean loss per batch 5.865541460656423e-10\n",
      "12-Mar-21 08:58:07 - epoch 619, mean loss per batch 7.20879515269369e-10\n",
      "12-Mar-21 08:58:09 - epoch 620, mean loss per batch 6.089416898075611e-10\n",
      "12-Mar-21 08:58:10 - epoch 621, mean loss per batch 7.110289707440004e-10\n",
      "12-Mar-21 08:58:11 - epoch 622, mean loss per batch 5.104364164249403e-10\n",
      "12-Mar-21 08:58:13 - epoch 623, mean loss per batch 5.507340299616158e-10\n",
      "12-Mar-21 08:58:14 - epoch 624, mean loss per batch 6.4028430420885e-10\n",
      "12-Mar-21 08:58:16 - epoch 625, mean loss per batch 6.44761801641499e-10\n",
      "12-Mar-21 08:58:17 - epoch 626, mean loss per batch 4.387962290528172e-10\n",
      "12-Mar-21 08:58:19 - epoch 627, mean loss per batch 4.701388092933976e-10\n",
      "12-Mar-21 08:58:20 - epoch 628, mean loss per batch 5.005858953850588e-10\n",
      "12-Mar-21 08:58:22 - epoch 629, mean loss per batch 4.1193114410984155e-10\n",
      "12-Mar-21 08:58:23 - epoch 630, mean loss per batch 4.656612958479164e-10\n",
      "12-Mar-21 08:58:24 - epoch 631, mean loss per batch 5.417790180159634e-10\n",
      "12-Mar-21 08:58:26 - epoch 632, mean loss per batch 5.373014992328715e-10\n",
      "12-Mar-21 08:58:27 - epoch 633, mean loss per batch 4.5581076733537995e-10\n",
      "12-Mar-21 08:58:29 - epoch 634, mean loss per batch 4.701388082258754e-10\n",
      "12-Mar-21 08:58:30 - epoch 635, mean loss per batch 4.2536368231124083e-10\n",
      "12-Mar-21 08:58:32 - epoch 636, mean loss per batch 5.28346470206865e-10\n",
      "12-Mar-21 08:58:34 - epoch 637, mean loss per batch 4.029761214889678e-10\n",
      "12-Mar-21 08:58:35 - epoch 638, mean loss per batch 3.358134379546266e-10\n",
      "12-Mar-21 08:58:36 - epoch 639, mean loss per batch 4.164086618254113e-10\n",
      "12-Mar-21 08:58:38 - epoch 640, mean loss per batch 3.537234842638963e-10\n",
      "12-Mar-21 08:58:39 - epoch 641, mean loss per batch 4.235726918783583e-10\n",
      "12-Mar-21 08:58:41 - epoch 642, mean loss per batch 2.8656079966203296e-10\n",
      "12-Mar-21 08:58:42 - epoch 643, mean loss per batch 3.4476846591311106e-10\n",
      "12-Mar-21 08:58:44 - epoch 644, mean loss per batch 5.220779577883241e-10\n",
      "12-Mar-21 08:58:45 - epoch 645, mean loss per batch 2.865607975269887e-10\n",
      "12-Mar-21 08:58:47 - epoch 646, mean loss per batch 4.952128984658799e-10\n",
      "12-Mar-21 08:58:48 - epoch 647, mean loss per batch 3.474549707778334e-10\n",
      "12-Mar-21 08:58:50 - epoch 648, mean loss per batch 3.0894836261935027e-10\n",
      "12-Mar-21 08:58:51 - epoch 649, mean loss per batch 2.632777318805752e-10\n",
      "12-Mar-21 08:58:53 - epoch 650, mean loss per batch 2.596957232592345e-10\n",
      "12-Mar-21 08:58:54 - epoch 651, mean loss per batch 2.5521821515136406e-10\n",
      "12-Mar-21 08:58:56 - epoch 652, mean loss per batch 2.5074070170588286e-10\n",
      "12-Mar-21 08:58:57 - epoch 653, mean loss per batch 2.9551582441795104e-10\n",
      "12-Mar-21 08:58:59 - epoch 654, mean loss per batch 2.990978373093803e-10\n",
      "12-Mar-21 08:59:00 - epoch 655, mean loss per batch 3.707380364242469e-10\n",
      "12-Mar-21 08:59:01 - epoch 656, mean loss per batch 2.014880623457671e-10\n",
      "12-Mar-21 08:59:03 - epoch 657, mean loss per batch 2.417856769499648e-10\n",
      "12-Mar-21 08:59:04 - epoch 658, mean loss per batch 1.83578012833931e-10\n",
      "12-Mar-21 08:59:06 - epoch 659, mean loss per batch 3.0447085557900194e-10\n",
      "12-Mar-21 08:59:07 - epoch 660, mean loss per batch 2.552182236915411e-10\n",
      "12-Mar-21 08:59:09 - epoch 661, mean loss per batch 2.373081656395279e-10\n",
      "12-Mar-21 08:59:10 - epoch 662, mean loss per batch 2.507406963682722e-10\n",
      "12-Mar-21 08:59:11 - epoch 663, mean loss per batch 1.7910050045597195e-10\n",
      "12-Mar-21 08:59:13 - epoch 664, mean loss per batch 2.543227124622678e-10\n",
      "12-Mar-21 08:59:14 - epoch 665, mean loss per batch 1.791005047260605e-10\n",
      "12-Mar-21 08:59:16 - epoch 666, mean loss per batch 1.7014547463253174e-10\n",
      "12-Mar-21 08:59:17 - epoch 667, mean loss per batch 2.1044308710168516e-10\n",
      "12-Mar-21 08:59:19 - epoch 668, mean loss per batch 1.611904509441358e-10\n",
      "12-Mar-21 08:59:20 - epoch 669, mean loss per batch 1.970105563729409e-10\n",
      "12-Mar-21 08:59:21 - epoch 670, mean loss per batch 1.3880288798681853e-10\n",
      "12-Mar-21 08:59:23 - epoch 671, mean loss per batch 1.4328040249982182e-10\n",
      "12-Mar-21 08:59:24 - epoch 672, mean loss per batch 2.014880655483335e-10\n",
      "12-Mar-21 08:59:26 - epoch 673, mean loss per batch 1.388028901218628e-10\n",
      "12-Mar-21 08:59:27 - epoch 674, mean loss per batch 1.1193781371906433e-10\n",
      "12-Mar-21 08:59:28 - epoch 675, mean loss per batch 1.298478642984226e-10\n",
      "12-Mar-21 08:59:30 - epoch 676, mean loss per batch 2.1850261557363984e-10\n",
      "12-Mar-21 08:59:31 - epoch 677, mean loss per batch 1.7014547997014245e-10\n",
      "12-Mar-21 08:59:33 - epoch 678, mean loss per batch 1.7462299234810147e-10\n",
      "12-Mar-21 08:59:34 - epoch 679, mean loss per batch 1.0298279003066842e-10\n",
      "12-Mar-21 08:59:36 - epoch 680, mean loss per batch 1.2089284594763738e-10\n",
      "12-Mar-21 08:59:37 - epoch 681, mean loss per batch 1.1641532823206765e-10\n",
      "12-Mar-21 08:59:38 - epoch 682, mean loss per batch 1.1193781478658647e-10\n",
      "12-Mar-21 08:59:40 - epoch 683, mean loss per batch 1.2537035512303e-10\n",
      "12-Mar-21 08:59:41 - epoch 684, mean loss per batch 1.164153239619791e-10\n",
      "12-Mar-21 08:59:43 - epoch 685, mean loss per batch 1.5223542512069563e-10\n",
      "12-Mar-21 08:59:44 - epoch 686, mean loss per batch 1.343253756088595e-10\n",
      "12-Mar-21 08:59:46 - epoch 687, mean loss per batch 1.2089284061002667e-10\n",
      "12-Mar-21 08:59:47 - epoch 688, mean loss per batch 8.507273624874373e-11\n",
      "12-Mar-21 08:59:48 - epoch 689, mean loss per batch 1.522354293907842e-10\n",
      "12-Mar-21 08:59:50 - epoch 690, mean loss per batch 8.507273731626587e-11\n",
      "12-Mar-21 08:59:51 - epoch 691, mean loss per batch 8.059522600582898e-11\n",
      "12-Mar-21 08:59:53 - epoch 692, mean loss per batch 1.4775791487778087e-10\n",
      "12-Mar-21 08:59:54 - epoch 693, mean loss per batch 9.402776527475035e-11\n",
      "12-Mar-21 08:59:56 - epoch 694, mean loss per batch 8.059522600582898e-11\n",
      "12-Mar-21 08:59:57 - epoch 695, mean loss per batch 8.059522707335112e-11\n",
      "12-Mar-21 08:59:59 - epoch 696, mean loss per batch 5.820766198098955e-11\n",
      "12-Mar-21 09:00:00 - epoch 697, mean loss per batch 2.3999468544956015e-10\n",
      "12-Mar-21 09:00:01 - epoch 698, mean loss per batch 1.029827921657127e-10\n",
      "12-Mar-21 09:00:03 - epoch 699, mean loss per batch 5.820766304851169e-11\n",
      "12-Mar-21 09:00:04 - epoch 700, mean loss per batch 6.268517542647072e-11\n",
      "12-Mar-21 09:00:06 - epoch 701, mean loss per batch 9.402776313970608e-11\n",
      "12-Mar-21 09:00:07 - epoch 702, mean loss per batch 6.716268887195188e-11\n",
      "12-Mar-21 09:00:09 - epoch 703, mean loss per batch 5.373015280559693e-11\n",
      "12-Mar-21 09:00:10 - epoch 704, mean loss per batch 6.268517435894858e-11\n",
      "12-Mar-21 09:00:11 - epoch 705, mean loss per batch 8.059522814087326e-11\n",
      "12-Mar-21 09:00:13 - epoch 706, mean loss per batch 7.164020338495519e-11\n",
      "12-Mar-21 09:00:14 - epoch 707, mean loss per batch 2.2387561889795158e-11\n",
      "12-Mar-21 09:00:16 - epoch 708, mean loss per batch 6.268517649399286e-11\n",
      "12-Mar-21 09:00:17 - epoch 709, mean loss per batch 7.611771576291423e-11\n",
      "12-Mar-21 09:00:19 - epoch 710, mean loss per batch 5.373015173807479e-11\n",
      "12-Mar-21 09:00:20 - epoch 711, mean loss per batch 7.164020231743305e-11\n",
      "12-Mar-21 09:00:21 - epoch 712, mean loss per batch 8.865474914017295e-11\n",
      "12-Mar-21 09:00:23 - epoch 713, mean loss per batch 3.134258984827964e-11\n",
      "12-Mar-21 09:00:24 - epoch 714, mean loss per batch 7.611771576291423e-11\n",
      "12-Mar-21 09:00:26 - epoch 715, mean loss per batch 2.6865075335276326e-11\n",
      "12-Mar-21 09:00:27 - epoch 716, mean loss per batch 4.477512698215673e-11\n",
      "12-Mar-21 09:00:29 - epoch 717, mean loss per batch 2.2387561889795158e-11\n",
      "12-Mar-21 09:00:30 - epoch 718, mean loss per batch 1.7910050579358263e-11\n",
      "12-Mar-21 09:00:32 - epoch 719, mean loss per batch 7.61177146953921e-11\n",
      "12-Mar-21 09:00:33 - epoch 720, mean loss per batch 5.373015067055265e-11\n",
      "12-Mar-21 09:00:34 - epoch 721, mean loss per batch 4.029761567171984e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV F1-score 0.9448405419280588\n",
      "TEST  F1-score 0.9326867630308513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.055159458071941225"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness([2500, 512, 722, 2, 2048, 256, 0.0001, 0.3948527313133352])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
